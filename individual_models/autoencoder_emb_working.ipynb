{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f013ca4e970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "def data_prep_(cryptic, gene_list, dr_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "    overlap = overlap['sample_id'].unique()\n",
    "    print(overlap)\n",
    "    print(overlap.shape)\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    crypticSNPnames = np.load('crypticSNPnames.npy', allow_pickle=True)\n",
    "    variants = variants[variants['SNP'].isin(crypticSNPnames)]\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        return output_list\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "    aa = []\n",
    "    dr = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "            if dr_list[0] in variants[variants['sample_id']==x]['drugs'].unique():\n",
    "                dr.append(1)\n",
    "            else:\n",
    "                dr.append(0)\n",
    "        else:\n",
    "            # aa.append([0]*len(all_snp))\n",
    "            pass\n",
    "        # print('SNP')\n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([“ENA_RUN”])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)return aa_array, dr#, mic_aa\n",
    "    return aa_array, dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_drs_cryptic_emb.npy')\n",
    "snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_snps_cryptic_emb.npy')\n",
    "\n",
    "drs = pd.DataFrame(drs)     \n",
    "train_data, test_data, train_target, test_target = data_split(snps, drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "cryptic_drs = pd.DataFrame(drs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0),  torch.tensor(mic).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "train_dataset = Dataset(train_data, train_target, one_hot_dtype=torch.float, transform=False)\n",
    "val_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "cryptic_dataset = Dataset(cryptic_snps, cryptic_drs, one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "#                                              test_size=0.1,\n",
    "#                                              random_state=42,\n",
    "#                                              shuffle=True,\n",
    "#                                              stratify=train_target)\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(cryptic_drs)),\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=42,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=cryptic_drs)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset_cryptic = Subset(cryptic_dataset, train_idx)\n",
    "test_dataset_cryptic = Subset(cryptic_dataset, validation_idx)    \n",
    "\n",
    "\n",
    "# # Subset dataset for train and val\n",
    "# train_dataset = Subset(training_dataset, train_idx)\n",
    "# val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn_mic = get_weighted_masked_cross_entropy_loss(column_weight_maps[0])\n",
    "# weighted_cross_entropy_loss_fn_bi = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_y'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder class\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 100),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, input_size),\n",
    "            nn.Sigmoid()  # Use Sigmoid since input is binary\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# criterion = F.cross_entropy\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Train Loss: 0.0049, Val Loss: 0.0045\n",
      "Epoch [20/150], Train Loss: 0.0044, Val Loss: 0.0044\n",
      "Epoch [30/150], Train Loss: 0.0040, Val Loss: 0.0041\n",
      "Epoch [40/150], Train Loss: 0.0037, Val Loss: 0.0035\n",
      "Epoch [50/150], Train Loss: 0.0031, Val Loss: 0.0028\n",
      "Epoch [60/150], Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Epoch [70/150], Train Loss: 0.0025, Val Loss: 0.0025\n",
      "Epoch [80/150], Train Loss: 0.0023, Val Loss: 0.0021\n",
      "Epoch [90/150], Train Loss: 0.0021, Val Loss: 0.0016\n",
      "Epoch [100/150], Train Loss: 0.0020, Val Loss: 0.0024\n",
      "Epoch [110/150], Train Loss: 0.0018, Val Loss: 0.0018\n",
      "Epoch [120/150], Train Loss: 0.0017, Val Loss: 0.0016\n",
      "Epoch [130/150], Train Loss: 0.0015, Val Loss: 0.0023\n",
      "Epoch [140/150], Train Loss: 0.0014, Val Loss: 0.0019\n",
      "Epoch [150/150], Train Loss: 0.0013, Val Loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Accuracy: 47.90%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01       209\n",
      "           1       0.48      0.88      0.62       709\n",
      "           2       0.45      0.12      0.19       545\n",
      "           3       0.49      0.42      0.46       272\n",
      "           4       0.45      0.58      0.51       232\n",
      "           5       0.54      0.48      0.51        81\n",
      "\n",
      "    accuracy                           0.48      2048\n",
      "   macro avg       0.57      0.42      0.38      2048\n",
      "weighted avg       0.53      0.48      0.40      2048\n",
      "\n",
      "Doubling Dilution Accuracy: 92.29%\n",
      "AUC: 0.8173499921739049\n",
      "Sensitivity: 0.7188498402555911\n",
      "Specificity: 0.915850144092219\n",
      "XGBoost Accuracy: 48.63%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.01      0.03       209\n",
      "           1       0.49      0.88      0.63       709\n",
      "           2       0.48      0.13      0.20       545\n",
      "           3       0.47      0.49      0.48       272\n",
      "           4       0.47      0.58      0.52       232\n",
      "           5       0.71      0.37      0.49        81\n",
      "\n",
      "    accuracy                           0.49      2048\n",
      "   macro avg       0.47      0.41      0.39      2048\n",
      "weighted avg       0.46      0.49      0.42      2048\n",
      "\n",
      "Doubling Dilution Accuracy: 92.53%\n",
      "AUC: 0.8059616429275119\n",
      "Sensitivity: 0.6805111821086262\n",
      "Specificity: 0.9314121037463977\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=8)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "\n",
    "train_loader_cryptic = DataLoader(dataset=train_dataset_cryptic, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "test_loader_cryptic = DataLoader(dataset=test_dataset_cryptic, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True) \n",
    "for lr in [1e-5]: \n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=True,\n",
    "    #     with_stack=True\n",
    "    # ) as prof:\n",
    "        # Parameters\n",
    "    input_size = 1473  # Adjust this based on your input vector length\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Instantiate the model, define loss function and optimizer\n",
    "    model = Autoencoder(input_size).to(device)  # Move model to GPU\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for data, _ in train_loader:\n",
    "            # print('new batch', data.shape)\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)  # Move data to GPU\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data)\n",
    "                val_loss.append(loss.item())\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}')\n",
    "\n",
    "# prof.export_chrome_trace(\"torch_trace.json\")\n",
    "\n",
    "# Plot train loss and val loss\n",
    "\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    test_compressed_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in train_loader_cryptic:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            \n",
    "        for data, label in test_loader_cryptic:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            test_compressed_data.append(compressed.cpu().numpy())\n",
    "            test_labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    test_compressed_data = np.vstack(test_compressed_data)\n",
    "    test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "    test_labels = np.hstack(test_labels)\n",
    "\n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "#MLP\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(500, 100, 100, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    target_min, target_max = labels.min(), labels.max()\n",
    "\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "    print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    #testing\n",
    "    cutoff = 4\n",
    "    test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.squeeze(np.array( y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)\n",
    "\n",
    "#XGB\n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred_xgb, y_test)])\n",
    "    print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    #testing\n",
    "    cutoff = 4\n",
    "    test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.squeeze(np.array( y_pred_xgb)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 92.83%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Autoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([10000, 1473]) from checkpoint, the shape in current model is torch.Size([5000, 1473]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([2000, 10000]) from checkpoint, the shape in current model is torch.Size([2000, 5000]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([10000, 2000]) from checkpoint, the shape in current model is torch.Size([5000, 2000]).\n\tsize mismatch for decoder.4.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for decoder.6.weight: copying a param with shape torch.Size([1473, 10000]) from checkpoint, the shape in current model is torch.Size([1473, 5000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32363/1016365165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load saved parameters into the model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoencoder.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m train_idx, validation_idx = train_test_split(np.arange(len(cryptic_drs)),\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1605\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Autoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([10000, 1473]) from checkpoint, the shape in current model is torch.Size([5000, 1473]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([2000, 10000]) from checkpoint, the shape in current model is torch.Size([2000, 5000]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([10000, 2000]) from checkpoint, the shape in current model is torch.Size([5000, 2000]).\n\tsize mismatch for decoder.4.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for decoder.6.weight: copying a param with shape torch.Size([1473, 10000]) from checkpoint, the shape in current model is torch.Size([1473, 5000])."
     ]
    }
   ],
   "source": [
    "input_size = 1473  \n",
    "\n",
    "model = Autoencoder(input_size).to(device)  # Move model to GPU\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Load saved parameters into the model instance\n",
    "model.load_state_dict(torch.load('autoencoder.pth'))\n",
    "\n",
    "train_loader_cryptic = DataLoader(dataset=train_dataset_cryptic, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "test_loader_cryptic = DataLoader(dataset=test_dataset_cryptic, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True) \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "compressed_data = []\n",
    "labels = []\n",
    "\n",
    "test_compressed_data = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())\n",
    "        \n",
    "    for data, label in test_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        test_compressed_data.append(compressed.cpu().numpy())\n",
    "        test_labels.append(label.numpy())\n",
    "\n",
    "compressed_data = np.vstack(compressed_data)\n",
    "compressed_data = compressed_data.squeeze(axis=1)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "test_compressed_data = np.vstack(test_compressed_data)\n",
    "test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "test_labels = np.hstack(test_labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500, 100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.01%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       206\n",
      "           1       0.40      0.91      0.55       715\n",
      "           2       0.00      0.00      0.00       528\n",
      "           3       0.00      0.00      0.00       271\n",
      "           4       0.26      0.43      0.33       244\n",
      "           5       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.37      2048\n",
      "   macro avg       0.11      0.22      0.15      2048\n",
      "weighted avg       0.17      0.37      0.23      2048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 46.53%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.01      0.03       206\n",
      "           1       0.49      0.89      0.63       715\n",
      "           2       0.37      0.12      0.18       528\n",
      "           3       0.45      0.44      0.44       271\n",
      "           4       0.42      0.46      0.44       244\n",
      "           5       0.63      0.29      0.39        84\n",
      "\n",
      "    accuracy                           0.47      2048\n",
      "   macro avg       0.44      0.37      0.35      2048\n",
      "weighted avg       0.43      0.47      0.39      2048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())\n",
    "        \n",
    "    for data, label in test_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        test_compressed_data.append(compressed.cpu().numpy())\n",
    "        test_labels.append(label.numpy())\n",
    "\n",
    "compressed_data = np.vstack(compressed_data)\n",
    "compressed_data = compressed_data.squeeze(axis=1)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "test_compressed_data = np.vstack(test_compressed_data)\n",
    "test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "test_labels = np.hstack(test_labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500, 100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n",
    "\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.48%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      1891\n",
      "           1       0.24      0.07      0.11       564\n",
      "\n",
      "    accuracy                           0.73      2455\n",
      "   macro avg       0.51      0.50      0.48      2455\n",
      "weighted avg       0.65      0.73      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 96.82%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1891\n",
      "           1       0.94      0.92      0.93       564\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.96      0.95      0.95      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "    model.eval()\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2455,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.66%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.87      1879\n",
      "           1       0.55      0.03      0.06       576\n",
      "\n",
      "    accuracy                           0.77      2455\n",
      "   macro avg       0.66      0.51      0.46      2455\n",
      "weighted avg       0.72      0.77      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 97.27%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1879\n",
      "           1       0.95      0.93      0.94       576\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.97      0.96      0.96      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performan\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autonencoder_model_state.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running models using the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(variants['sample_id'].unique()):\n",
    "        aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "\n",
    "    return aa_array, mic_aa\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29397/2926147983.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb.loc[i, f'{x}'] = '16'\n",
      "/tmp/ipykernel_29397/2926147983.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float')\n",
      "100%|██████████| 11362/11362 [01:01<00:00, 184.30it/s]\n",
      "/tmp/ipykernel_29397/3522971869.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
      "/tmp/ipykernel_29397/3522971869.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "gene_list = ['embB', 'embA', 'embC']\n",
    "dr_list = ['ethambutol']\n",
    "df_emb = df[df['EMB_MIC'].isin(['>8','8.0', '4.0', '2.0', '1.0', '0.5'])]\n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "for i, row in df_emb.iterrows():\n",
    "    x = 'EMB_MIC'\n",
    "    if row[x] == '>8' :\n",
    "        df_emb.loc[i, f'{x}'] = '16'\n",
    "    elif row[x] == '<=0.25':\n",
    "        df_emb.loc[i, f'{x}'] = '0.125'\n",
    "        \n",
    "df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float') \n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "\n",
    "# variants = pd.read_csv('variants_full.csv')\n",
    "# variants = variants[variants['type'] != 'synonymous_variant']\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['LOW','MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[df_emb['ENA_RUN'].isin(samples)]\n",
    "cryptic = df_emb\n",
    "aa_array, drs = data_prep_(cryptic, gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11362, 1710)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "aa_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(variants['sample_id'].unique()):\n",
    "        aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "\n",
    "    return aa_array, mic_aa\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array#, mic_aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
