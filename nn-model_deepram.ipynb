{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-g1/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1c05409a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts_list(lst):\n",
    "    \"\"\"\n",
    "    Computes the frequency count of unique elements in a list and returns a dictionary, sorted by frequency count in\n",
    "    descending order.\n",
    "\n",
    "    Args:\n",
    "    - lst (list): List of elements\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with unique elements as keys and their frequency count as values, sorted by frequency count\n",
    "    in descending order\n",
    "    \"\"\"\n",
    "    value_counts = {}\n",
    "    for item in lst:\n",
    "        if item in value_counts:\n",
    "            value_counts[item] += 1\n",
    "        else:\n",
    "            value_counts[item] = 1\n",
    "    sorted_value_counts = dict(sorted(value_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_value_counts\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/gene_seq_train.csv')\n",
    "train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/res_train.csv')\n",
    "#don't touch test data, split out validation data from training data during training\n",
    "test_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/gene_seq_test.csv')\n",
    "test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/res_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = train_data.shape[0]\n",
    "DRUGS = ['AMIKACIN',\n",
    " 'CAPREOMYCIN',\n",
    " 'CIPROFLOXACIN',\n",
    " 'ETHAMBUTOL',\n",
    " 'ETHIONAMIDE',\n",
    " 'ISONIAZID',\n",
    " 'KANAMYCIN',\n",
    " 'LEVOFLOXACIN',\n",
    " 'MOXIFLOXACIN',\n",
    " 'OFLOXACIN',\n",
    " 'PYRAZINAMIDE',\n",
    " 'RIFAMPICIN',\n",
    " 'STREPTOMYCIN']\n",
    "\n",
    "DRUGS = train_target.columns\n",
    "LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_torch(seq: str, dtype=torch.int8):\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    acgt_bytes = torch.ByteTensor(list(bytes(\"ACGT\", \"utf-8\")))\n",
    "    arr = torch.zeros(4, (len(seq_bytes)), dtype=dtype)\n",
    "    arr[0, seq_bytes == acgt_bytes[0]] = 1\n",
    "    arr[1, seq_bytes == acgt_bytes[1]] = 1\n",
    "    arr[2, seq_bytes == acgt_bytes[2]] = 1\n",
    "    arr[3, seq_bytes == acgt_bytes[3]] = 1\n",
    "    return arr\n",
    "\n",
    "# def one_hot_torch(seq):\n",
    "#     oh = []\n",
    "#     for sample in seq:\n",
    "#         sample = torch.ByteTensor(list(bytes(sample, \"utf-8\")))\n",
    "#         acgt_bytes = torch.ByteTensor(list(bytes(\"ACGT\", \"utf-8\")))\n",
    "#         arr = torch.zeros((len(sample), 4), dtype=torch.int8)\n",
    "#         arr[sample == acgt_bytes[0], 0] = 1\n",
    "#         arr[sample == acgt_bytes[1], 1] = 1\n",
    "#         arr[sample == acgt_bytes[2], 2] = 1\n",
    "#         arr[sample == acgt_bytes[3], 3] = 1\n",
    "#         oh.append(arr)\n",
    "#     return torch.stack(oh)\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "def get_masked_loss(loss_fn):\n",
    "    \"\"\"\n",
    "    Returns a loss function that ignores NaN values\n",
    "    \"\"\"\n",
    "\n",
    "    def masked_loss(y_true, y_pred):\n",
    "        y_pred = y_pred.view(-1, 13)  # Ensure y_pred has the same shape as y_true and non_nan_mask\n",
    "        # ic(y_pred.shape)\n",
    "        # ic(y_true.shape)\n",
    "        non_nan_mask = ~y_true.isnan()\n",
    "        # ic(non_nan_mask)\n",
    "        y_true_non_nan = y_true[non_nan_mask]\n",
    "        y_pred_non_nan = y_pred[non_nan_mask]\n",
    "\n",
    "        return loss_fn(y_pred_non_nan, y_true_non_nan)\n",
    "\n",
    "    return masked_loss\n",
    "\n",
    "masked_MSE = get_masked_loss(torch.nn.MSELoss())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julian's code - implement this, might be faster\n",
    "class OneHotSeqsDataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "    ):\n",
    "        self.seq_df = seq_df[target_loci]\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        if not self.seq_df.index.equals(self.res_df.index):\n",
    "            raise ValueError(\n",
    "                \"Indices of sequence and resistance dataframes don't match up\"\n",
    "            )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df.iloc[index].str.cat()\n",
    "            res = self.res_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df.loc[index].str.cat()\n",
    "            res = self.res_df.loc[index]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "        return one_hot_torch(seqs_comb, dtype=self.one_hot_dtype), torch.tensor(res)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "    \n",
    "training_dataset = OneHotSeqsDataset(train_data, train_target, one_hot_dtype=torch.float)\n",
    "train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.8), len(training_dataset)-int(len(training_dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deepRam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, nummotif,motiflen,RNN_hidden_size,hidden_size,hidden,dropprob,sigmaConv,sigmaNeu,sigmaRNN,xavier_init):\n",
    "      \n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.hidden=hidden\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        \n",
    "        self.dropprob=dropprob\n",
    "        self.sigmaConv=sigmaConv\n",
    "        self.sigmaNeu=sigmaNeu\n",
    "        self.hidden_size=hidden_size  \n",
    "        self.input_channels=4\n",
    "        \n",
    "        # Embedding\n",
    "        if embedding:\n",
    "              model1 = gensim.models.Word2Vec.load(word2vec_model)\n",
    "              weights = torch.FloatTensor(model1.wv.vectors)\n",
    "              self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "              self.input_channels=Embsize\n",
    "              \n",
    "        # Convnet\n",
    "        self.ConvWeights=[]\n",
    "        self.ConvBias=[]\n",
    "        self.wConv=torch.randn(nummotif,self.input_channels,motiflen).to(device)\n",
    "        self.wRect=torch.randn(nummotif).to(device)\n",
    "        self.ConvWeights.append(self.wConv)\n",
    "        self.ConvBias.append(self.wRect)\n",
    "        conv_channels=nummotif\n",
    "        \n",
    "        if conv:\n",
    "              self.FC_size= nummotif\n",
    "              self.input_channels=nummotif\n",
    "              for c in range(1,conv_layers):\n",
    "                  Wconv=torch.randn(int(1.5*c*nummotif),conv_channels,motiflen).to(device)\n",
    "                  Bconv=torch.randn(int(1.5*c*nummotif)).to(device)\n",
    "                  self.ConvWeights.append(Wconv)\n",
    "                  self.ConvBias.append(Bconv)\n",
    "                  conv_channels=int(1.5*c*nummotif) # number of channels in the next layer\n",
    "                  self.FC_size= int(1.5*c*nummotif)\n",
    "                  self.input_channels=int(1.5*c*nummotif)\n",
    "              torch.nn.init.normal_(self.ConvWeights[0],mean=0,std=sigmaConv)\n",
    "              \n",
    "              ind=0\n",
    "              for weights in self.ConvWeights:\n",
    "                  weights.requires_grad=True\n",
    "                  if ind>0:\n",
    "                    if dilation>1:\n",
    "                      torch.nn.init.normal_(weights,mean=0,std=0.1)\n",
    "                      print('ffffffff')\n",
    "                    else:\n",
    "                      torch.nn.init.xavier_uniform_(weights)\n",
    "                  ind=ind+1\n",
    "              for weights in self.ConvBias:\n",
    "                  weights.requires_grad=True\n",
    "                  torch.nn.init.normal_(weights)\n",
    "            \n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "        if RNN:\n",
    "              if RNN_type=='GRU':\n",
    "                  self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=False).to(device)\n",
    "                  self.FC_size= RNN_hidden_size\n",
    "              elif RNN_type=='BiGRU':\n",
    "                  self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=True).to(device)\n",
    "                  self.FC_size= 2*RNN_hidden_size\n",
    "              elif RNN_type=='LSTM':\n",
    "                  self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=False).to(device)\n",
    "                  self.FC_size= RNN_hidden_size\n",
    "              elif RNN_type=='BiLSTM':\n",
    "                  self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=True).to(device)\n",
    "                  self.FC_size= 2*RNN_hidden_size\n",
    "              if not xavier_init:\n",
    "                for layer_p in self.rnn._all_weights:\n",
    "                  for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                      torch.nn.init.normal_(self.rnn.__getattr__(p),mean=0,std=sigmaRNN)\n",
    "              else:\n",
    "                for layer_p in self.rnn._all_weights:\n",
    "                  for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                      torch.nn.init.xavier_uniform_(self.rnn.__getattr__(p))\n",
    "\n",
    "        # FC    \n",
    "        self.wHidden=torch.randn(self.FC_size,self.hidden_size).to(device)\n",
    "        self.wHiddenBias=torch.randn(self.hidden_size).to(device)\n",
    "        self.wHidden.requires_grad=True\n",
    "        self.wHiddenBias.requires_grad=True\n",
    "        \n",
    "        if not self.hidden:\n",
    "            self.wNeu=torch.randn(self.FC_size,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            if not xavier_init:\n",
    "              torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "            else:\n",
    "              torch.nn.init.xavier_uniform_(self.wNeu)\n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "           \n",
    "            self.wNeu=torch.randn(self.hidden_size,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)     \n",
    "            if not xavier_init:  \n",
    "              torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wHidden,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wHiddenBias,mean=0,std=self.sigmaNeu)\n",
    "            else:\n",
    "              torch.nn.init.xavier_uniform(self.wNeu)\n",
    "              torch.nn.init.xavier_uniform(self.wHidden)\n",
    "            \n",
    "  \n",
    "           \n",
    "\n",
    "        self.wNeu.requires_grad=True\n",
    "        self.wNeuBias.requires_grad=True\n",
    "        \n",
    "        self.dropout=torch.nn.Dropout(p=dropprob, inplace=False)\n",
    "        self.max=torch.nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        \n",
    "    def get_weights(self):\n",
    "        ll=[]\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "          for p in layer_p:\n",
    "            if 'weight' in p:\n",
    "               ll.append(self.rnn.__getattr__(p))\n",
    "        return ll+self.ConvWeights+self.ConvBias\n",
    "        \n",
    "    def layer1out(self,x):\n",
    "\t\t\n",
    "        if type(x) is np.ndarray:\n",
    "          x = torch.from_numpy(x.astype(np.float32))\n",
    "        #x = Variable(x, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "          x = x.to(device)\n",
    "        if embedding:\n",
    "          print(x.shape)\n",
    "          x= self.embedding(x)\n",
    "          x=x.permute(0,2,1)\n",
    "          print(x.shape)\n",
    "        if conv:\n",
    "          x=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
    "          out=x.clamp(min=0)\n",
    "          print(out.shape)\n",
    "          temp = out.data.cpu().numpy()\n",
    "        else:\n",
    "          print('you need to have CNN to visualize motifs')\n",
    "        return temp\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        if embedding:\n",
    "          # shape of x : batch_size x seq_len\n",
    "          x= self.embedding(x)\n",
    "          x=x.permute(0,2,1)\n",
    "          # shape of x_emb : batch_size x embd_size x seq_len\n",
    "          \n",
    "#         else:\n",
    "#           # shape of x : batch_size x 4 x seq_len\n",
    "         \n",
    "        if conv:\n",
    "          x=F.conv1d(x, self.ConvWeights[0], bias=self.ConvBias[0], stride=1, padding=0)\n",
    "          x=x.clamp(min=0)\n",
    "          x=self.max(x)\n",
    "          \n",
    "          for c in range(1,len(self.ConvWeights)):\n",
    "            x=F.conv1d(x, self.ConvWeights[c], bias=self.ConvBias[c], stride=1, padding=0,dilation=dilation)\n",
    "            x=x.clamp(min=0)\n",
    "            x=self.max(x)\n",
    "            \n",
    "        if RNN:\n",
    "          if conv:\n",
    "            \n",
    "            x=self.dropout(x)\n",
    "          x=x.permute(2,0,1)\n",
    "          # shape of x :  seq_len x batch_size  x features\n",
    "          output, _ = self.rnn(x)\n",
    "          # shape of output :  seq_len x batch_size  x num_directions * features\n",
    "          \n",
    "          if RNN_type== 'BiLSTM' or RNN_type=='BiGRU':\n",
    "\n",
    "              Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "              Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "              x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "              x=self.dropout(x)\n",
    "              #shape of x: batch_size x 2*hidden_size\n",
    "#               print(x.shape)\n",
    "              \n",
    "          else:\n",
    "                      ## from (1, N, hidden) to (N, hidden)\n",
    "              x = output[-1, :, :]\n",
    "              x=self.dropout(x)\n",
    "#               print(hn.shape)\n",
    "#               x = hn.view(hn.size()[1], hn.size(2))\n",
    "              # shape of x: batch_size x hidden_size\n",
    "              #print(x.shape)\n",
    "          \n",
    "          \n",
    "        else:\n",
    "          x, _ = torch.max(x, dim=2)\n",
    "          #print(x.shape)\n",
    "\n",
    "          # shape of x : batch_size x numb_filters\n",
    "          x=self.dropout(x)\n",
    "        if self.hidden:\n",
    "          x=x @ self.wHidden\n",
    "          x.add_(self.wHiddenBias)\n",
    "          x=x.clamp(min=0)\n",
    "          x=self.dropout(x)\n",
    "        x=x @ self.wNeu\n",
    "        x.add_(self.wNeuBias)\n",
    "          \n",
    "        \n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedding=False\n",
    "conv=True\n",
    "RNN=True\n",
    "RNN_type='BiLSTM'\n",
    "\n",
    "bases='ACGT' #DNA bases\n",
    "basesRNA='ACGU'#RNA bases\n",
    "batch_size=128\n",
    "evaluate_performance=False\n",
    "train=True\n",
    "model_dir='models/'\n",
    "# embedding hyper-parameters\n",
    "Embepochs=100 #number of epochs to train word2vec\n",
    "Embsize=50 # \n",
    "kmer_len=3\n",
    "stride=1\n",
    "word2vect_train=True\n",
    "word2vec_model='models/word2vec_model'\n",
    "# CNN hyper-parameters\n",
    "nummotif=16 #number of motifs to discover\n",
    "motiflen=24\n",
    "sigmaNeu=10**-6\n",
    "sigmaConv = 10**-6\n",
    "sigmaNeu = 10**-3\n",
    "sigmaRNN = 10**-4\n",
    "xavier_init=True\n",
    "RNN_hidden_size = 64\n",
    "hidden_size = 64 \n",
    "hidden = False\n",
    "dropprob = 0.2\n",
    "\n",
    "######### Global variables #########\n",
    "\n",
    "global embedding\n",
    "global conv\n",
    "global RNN\n",
    "global RNN_type\n",
    "global kmer_len\n",
    "global stride\n",
    "global word2vec_train \n",
    "global word2vec_model \n",
    "global evaluate_performance\n",
    "global train\n",
    "global model_dir\n",
    "global model_path\n",
    "global out_file\n",
    "global motif\n",
    "global motif_dir\n",
    "global tomtom_dir\n",
    "global data_type\n",
    "global conv_layers\n",
    "global RNN_layers\n",
    "global dilation\n",
    "\n",
    "# Default values for global variables\n",
    "embedding = False\n",
    "conv = True\n",
    "RNN = True\n",
    "RNN_type = 'GRU'  # Common types are 'GRU', 'LSTM', 'BiGRU', 'BiLSTM'\n",
    "kmer_len = 3  # Common k-mer lengths are 3, 4, or 5\n",
    "stride = 1\n",
    "word2vec_train = False\n",
    "word2vec_model = 'word2vec.model'  # Default path to a Word2Vec model file\n",
    "evaluate_performance = False\n",
    "train = True\n",
    "model_dir = 'models/'  # Default directory to save models\n",
    "model_path = 'models/model.pth'  # Default path to save a specific model\n",
    "out_file = 'output.txt'  # Default output file name\n",
    "motif = False\n",
    "motif_dir = 'motifs/'  # Default directory for motifs\n",
    "tomtom_dir = 'tomtom/'  # Default directory for tomtom tool outputs\n",
    "data_type = 'sequence'  # Common types might be 'sequence', 'image', 'text', etc.\n",
    "conv_layers = 2  # Default number of convolutional layers\n",
    "RNN_layers = 1  # Default number of RNN layers\n",
    "dilation = 1  # Default dilation value in convolutional layers\n",
    "\n",
    "deepRam = Network(nummotif,motiflen,RNN_hidden_size,hidden_size,hidden,dropprob,sigmaConv,sigmaNeu,sigmaRNN,xavier_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration():\n",
    "\tprint('start')\n",
    "\tbest_AUC=0\n",
    "\tdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\tprint(device)\n",
    "\t# device='cpu'\n",
    "\tlearning_steps_list=[5000,10000,15000,20000,25000,30000,35000,40000]\n",
    "\tfor number in range(40):\n",
    "\t\t# hyper-parameters\n",
    "\t\tRNN_hidden_size_list=[20,50,80,100]\n",
    "\t\tRNN_hidden_size=random.choice(RNN_hidden_size_list)\n",
    "\t\tdropoutList=[0,0.15,0.3,0.45,0.6] \n",
    "\t\tdropprob=random.choice(dropoutList)\n",
    "\t\thidden_list=[True,False]\n",
    "\t\thidden=random.choice(hidden_list)\n",
    "\t\txavier_List=[True,True,False] \n",
    "\t\txavier=random.choice(xavier_List)\n",
    "\t\thidden_size_list=[32,64]\n",
    "\t\thidden_size=random.choice(hidden_size_list)\n",
    "\t\toptim_list=['SGD','Adagrad','Adagrad']\n",
    "\t\toptim=random.choice(optim_list)\n",
    "\t\tlearning_rate=logsampler(0.005,0.5) \n",
    "\t\tmomentum_rate=sqrtsampler(0.95,0.99)  \n",
    "\t\tsigmaConv=logsampler(10**-6,10**-2)   \n",
    "\t\tsigmaNeu=logsampler(10**-3,10**-1) \n",
    "\t\tsigmaRNN=logsampler(10**-4,10**-1) \n",
    "\t\tweightDecay=logsampler(10**-10,10**-1) \n",
    "\t\tnummotif_list=[16]\n",
    "\t\tnummotif1=random.choice(nummotif_list)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\n",
    "\t\t\n",
    "\t\tmodel_auc=[[],[],[]]\n",
    "\t\tfor kk in range(3):\n",
    "\t\t\tmodel = Network(nummotif1,motiflen,RNN_hidden_size,hidden_size,hidden,dropprob,sigmaConv,sigmaNeu,sigmaRNN,xavier).to(device)\n",
    "\t\t\tif optim=='SGD':\n",
    "\t\t\t\toptimizer = torch.optim.SGD(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=learning_rate,momentum=momentum_rate,nesterov=True\n",
    "\t\t\t\t\t\t\t\t\t\t\t,weight_decay=weightDecay)\n",
    "\t\t\telse:\n",
    "\t\t\t\toptimizer = torch.optim.Adagrad(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=learning_rate,weight_decay=weightDecay)\n",
    "\n",
    "\t\t\ttrain_loader=train_dataloader[kk]\n",
    "\t\t\tvalid_loader=valid_dataloader[kk]\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tlearning_steps=0\n",
    "\t\t\twhile learning_steps<=40000:\n",
    "\t\t\t   \n",
    "\t\t\t\tauc=[]\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\t\tfor i, (data, target) in enumerate(train_loader):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tdata = data.to(device)\n",
    "\t\t\t\t\ttarget = target.to(device)\n",
    "\n",
    "\t\t\t\t\t# Forward pass\n",
    "\t\t\t\t\toutput = model(data)          \n",
    "\t\t\t\t\tloss = F.binary_cross_entropy(output,target)\n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\t\t\t\t\tlearning_steps+=1\n",
    "\n",
    "\t\t\t\t\tif learning_steps% 5000==0:\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t\t\tmodel.eval()\n",
    "\t\t\t\t\t\t\tauc=[]\n",
    "\t\t\t\t\t\t\tfor j, (data1, target1) in enumerate(valid_loader):\n",
    "\t\t\t\t\t\t\t\tdata1 = data1.to(device)\n",
    "\t\t\t\t\t\t\t\ttarget1 = target1.to(device)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t# Forward pass\n",
    "\t\t\t\t\t\t\t\toutput = model(data1)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tpred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "\t\t\t\t\t\t\t\tlabels=target1.cpu().numpy().reshape(output.shape[0])\n",
    "\t\t\t\t\t\t\t\tif output.shape[0]>60:\n",
    "\t\t\t\t\t\t\t\t\tauc.append(metrics.roc_auc_score(labels, pred))\n",
    "\t\t\t\t\t\t\t#print(np.mean(auc))\n",
    "\t\t\t\t\t\t\tmodel_auc[kk].append(np.mean(auc))\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tmodel.train()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\n",
    "\t\tprint('                   ##########################################               ')\n",
    "\n",
    "\t\tfor n in range(8):\n",
    "\t\t\tAUC=(model_auc[0][n]+model_auc[1][n]+model_auc[2][n])/3\n",
    "\t\t\t#print(AUC)\n",
    "\t\t\tif AUC>best_AUC:\n",
    "\t\t\t\tbest_AUC=AUC\n",
    "\t\t\t\tbest_learning_steps=learning_steps_list[n]\n",
    "\t\t\t\tbest_LearningRate=learning_rate\n",
    "\t\t\t\tbest_LearningMomentum=momentum_rate\n",
    "\t\t\t\tbest_sigmaConv=sigmaConv\n",
    "\t\t\t\tbest_dropprob=dropprob\n",
    "\t\t\t\tbest_sigmaNeu=sigmaNeu\n",
    "\t\t\t\tbest_RNN_hidden_size=RNN_hidden_size\n",
    "\t\t\t\tbest_weightDecay=weightDecay\n",
    "\t\t\t\tbest_hidden=hidden\n",
    "\t\t\t\tbest_sigmaRNN=sigmaRNN\n",
    "\t\t\t\tbest_xavier=xavier\n",
    "\t\t\t\tbest_optim=optim\n",
    "\t\t\t\tbest_nummotif=nummotif\n",
    "\t\t\t\tbest_hidden_size=hidden_size\n",
    "\n",
    "\n",
    "\tprint('best_AUC=',best_AUC)            \n",
    "\tprint('best_learning_steps=',best_learning_steps)      \n",
    "\tprint('best_LearningRate=',best_LearningRate)\n",
    "\tprint('best_LearningMomentum=',best_LearningMomentum)\n",
    "\tprint('best_sigmaConv=',best_sigmaConv)\n",
    "\tprint('best_dropprob=',best_dropprob)\n",
    "\tprint('best_sigmaNeu=',best_sigmaNeu)\n",
    "\tprint('best_RNN_hidden_size',best_RNN_hidden_size)\n",
    "\tprint('best_weightDecay=',weightDecay)\n",
    "\tprint('best_hidden=',best_hidden)\n",
    "\tprint('best_sigmaRNN=',best_sigmaRNN)\n",
    "\tprint('best_xavier=',best_xavier)\n",
    "\tprint('best_optim=',best_optim)\n",
    "\tprint('best_nummotif=',best_nummotif)\n",
    "\tprint('best_hidden_size=',best_hidden_size)\n",
    "\t\n",
    "\tbest_hyperparameters = {'best_learning_steps': best_learning_steps,'best_LearningRate':best_LearningRate,'best_LearningMomentum':best_LearningMomentum,'best_sigmaConv':best_sigmaConv,\n",
    "\t                         'best_dropprob':best_dropprob,'best_sigmaNeu':best_sigmaNeu,'best_RNN_hidden_size':best_RNN_hidden_size,\n",
    "\t                         'best_weightDecay':best_weightDecay,'best_hidden':best_hidden,'best_sigmaRNN':best_sigmaRNN,'best_xavier':best_xavier,'best_optim':best_optim,'best_nummotif':best_nummotif,'best_hidden_size':best_hidden_size}\n",
    "\ttorch.save(best_hyperparameters, model_dir+'best_hyperpamarameters.pth')\n",
    "\treturn best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deepram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyexpat.model' has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23631/3911165696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyexpat.model' has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "ic.disable()\n",
    "# ic.enable()\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x_batch = torch.squeeze(x, 0).to(device)\n",
    "        y_batch = y.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        # y_batch = y_batch.view(-1)\n",
    "\n",
    "        # y_batch = one_hot_torch(y).to(device)\n",
    "        # print('batch y size before flatten:',y_batch.size())\n",
    "        # y_batch = y_batch.flatten()\n",
    "        # print('batch y size after flatten:',y_batch.size())\n",
    "        # print(x_batch.size())\n",
    "        # print(x_batch.size())\n",
    "# For example, if you have a convolutional layer with 64 output channels, 3 input channels, and a kernel size of 3x3, the weight parameters would have a dimension of (64, 3, 3, 3)\n",
    "        # print(x_batch.size())\n",
    "        pred = deepRam(x_batch.float())\n",
    "        # print(x_batch)\n",
    "        # print(pred)\n",
    "        # pred = pred.unsqueeze(0)\n",
    "        # ic(pred)\n",
    "        # ic(y_batch)\n",
    "        loss_train = criterion(y_batch, pred)\n",
    "        ic(loss_train)\n",
    "        train_batch_loss.append(loss_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f'Batch - GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB')\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    with torch.no_grad():\n",
    "        # print('test')\n",
    "        for x, y in test_loader:\n",
    "            x_batch = x.to(device)\n",
    "            y_batch = y.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            # pred = pred.unsqueeze(0)\n",
    "\n",
    "            loss_test = criterion(y_batch, pred)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-g1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
