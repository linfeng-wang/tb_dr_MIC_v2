{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f50be88e030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts_list(lst):\n",
    "    \"\"\"\n",
    "    Computes the frequency count of unique elements in a list and returns a dictionary, sorted by frequency count in\n",
    "    descending order.\n",
    "\n",
    "    Args:\n",
    "    - lst (list): List of elements\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with unique elements as keys and their frequency count as values, sorted by frequency count\n",
    "    in descending order\n",
    "    \"\"\"\n",
    "    value_counts = {}\n",
    "    for item in lst:\n",
    "        if item in value_counts:\n",
    "            value_counts[item] += 1\n",
    "        else:\n",
    "            value_counts[item] = 1\n",
    "    sorted_value_counts = dict(sorted(value_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_value_counts\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 2000)\n",
    "    pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    pd.reset_option('display.float_format')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11920, 869)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.015</th>\n",
       "      <th>0.03</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>16.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11920 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.015  0.03  0.06  0.12  0.25  0.5  1.0  2.0  4.0  8.0  16.0\n",
       "0          0     0     0     1     0    0    0    0    0    0     0\n",
       "1          0     1     0     0     0    0    0    0    0    0     0\n",
       "2          0     0     0     0     1    0    0    0    0    0     0\n",
       "3          0     1     0     0     0    0    0    0    0    0     0\n",
       "4          0     0     0     0     0    0    0    0    1    0     0\n",
       "...      ...   ...   ...   ...   ...  ...  ...  ...  ...  ...   ...\n",
       "11915      0     0     0     0     0    0    0    0    0    0     1\n",
       "11916      0     0     0     0     0    0    0    0    0    1     0\n",
       "11917      1     0     0     0     0    0    0    0    0    0     0\n",
       "11918      0     0     0     0     0    0    0    0    0    1     0\n",
       "11919      0     0     0     0     0    0    0    0    0    1     0\n",
       "\n",
       "[11920 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_aa/RIF_MIC_class.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('data_aa/aa_rpoB.csv', header=None)\n",
    "# original_target = pd.read_csv('data_aa/RIF_MIC.csv', header=None)\n",
    "original_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_aa/RIF_MIC_class.csv')\n",
    "data = original_data\n",
    "\n",
    "target = original_target\n",
    "\n",
    "train_data_index = np.random.choice(data.shape[0], size=int(data.shape[0]*0.8), replace=False)\n",
    "all_indices = np.arange(data.shape[0])\n",
    "test_data_index = np.setdiff1d(all_indices, train_data_index)\n",
    "\n",
    "train_data = data.iloc[train_data_index,:]\n",
    "train_target = target.iloc[train_data_index,:]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "train_target = train_target.reset_index(drop=True)\n",
    "#don't touch test data, split out validation data from training data during training\n",
    "test_data = data.iloc[test_data_index,:]\n",
    "test_target = target.iloc[test_data_index,:]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "test_target = test_target.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df,\n",
    "        mic_df,\n",
    "        transform = None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.train_df = train_df\n",
    "        self.mic_df = mic_df\n",
    "        if not self.train_df.index.equals(self.mic_df.index):\n",
    "            raise ValueError(\n",
    "                \"Indices of training data and resistance data don't match up\"\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        if isinstance(index, int):\n",
    "            train = self.train_df.iloc[index]\n",
    "            mic = self.mic_df.loc[index]\n",
    "            \n",
    "        elif isinstance(index, str):\n",
    "            trains = self.train_df.loc[index]\n",
    "            mic = self.mic_df.loc[index].type(torch.int64)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "        \n",
    "        if self.transform:\n",
    "            self.mic_mean = self.mic_df.mean()\n",
    "            self.mic_std = self.mic_df.std()\n",
    "            mic = (mic - self.mic_mean) / self.mic_std\n",
    "        \n",
    "        return  torch.tensor(train),  torch.tensor(mic)\n",
    "    def __len__(self):\n",
    "        return self.mic_df.shape[0]\n",
    "    \n",
    "training_dataset = Dataset(train_data, train_target, transform=False)\n",
    "train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.8), len(training_dataset)-int(len(training_dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_target.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_loss(loss_fn):\n",
    "    \"\"\"\n",
    "    Returns a loss function that ignores NaN values\n",
    "    \"\"\"\n",
    "\n",
    "    def masked_loss(y_true, y_pred):\n",
    "        y_pred = y_pred.view(-1, 1)  # Ensure y_pred has the same shape as y_true and non_nan_mask\n",
    "        # ic(y_true)\n",
    "        non_nan_mask = ~y_true.isnan()\n",
    "        # ic(non_nan_mask)\n",
    "        y_true_non_nan = y_true[non_nan_mask]\n",
    "        y_pred_non_nan = y_pred[non_nan_mask]\n",
    "\n",
    "        return loss_fn(y_pred_non_nan, y_true_non_nan)\n",
    "\n",
    "    return masked_loss\n",
    "\n",
    "masked_MSE = get_masked_loss(torch.nn.MSELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channel = 869, first_h_layer = 469, out_channel=1, batch_size=1, dropout_rate=0.0, num_dense_layers=3, filter_scaling_factor=1.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_channel = in_channel\n",
    "        self.first_h_layer = first_h_layer\n",
    "        self.out_channel = out_channel\n",
    "        self.dense_dropout_rate = dropout_rate\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.filter_scaling_factor=filter_scaling_factor\n",
    "        \n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        for i in range(self.num_dense_layers):\n",
    "            layer = self._dense_layer(100,100)\n",
    "            self.dense_layers.append(layer)\n",
    "        \n",
    "        # current_num_filters = self.cnn_output_size(self.in_channel, 12)\n",
    "        current_num_filters = self.cnn_output_size_multilayer(self.in_channel, 12, num_layers=2)\n",
    "        num_dense_neurons = 100\n",
    "        num_dense_layers = 2\n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [current_num_filters]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        )\n",
    "            # current_num_filters = int(current_num_filters * filter_scaling_factor)\n",
    "\n",
    "        # self.feature_extraction = nn.Conv1d(in_channels, hidden, kernel_size=kernel_size),]\n",
    "        # self.starting_layers = nn.Sequential(\n",
    "        #     nn.Linear(self.in_channel, self.first_h_layer),\n",
    "        #     nn.BatchNorm1d(self.first_h_layer),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(self.dense_dropout_rate),  # Dropout layer after the first ReLU\n",
    "        #     nn.Linear(self.first_h_layer, 100),\n",
    "        #     nn.BatchNorm1d(100),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(self.dense_dropout_rate))  # Dropout layer after the first ReLU\n",
    "        \n",
    "        self.starting_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 1, 12),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dense_dropout_rate),  # Dropout layer after the first ReLU\n",
    "            nn.Conv1d(1, 1, 12),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dense_dropout_rate))  # Dropout layer after the first ReLU\n",
    "            \n",
    "        self.out_layer = nn.Sequential(nn.Linear(100, self.out_channel), nn.Sigmoid())\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    # def cnn_output_size(self, n, f, p = 0, s = 1):\n",
    "    #     '''\n",
    "    #     n: input size\n",
    "    #     f: kernel size\n",
    "    #     p: padding\n",
    "    #     s: stride\n",
    "    #     '''\n",
    "    #     return int((n - f + 2 * p) / s) + 1\n",
    "    \n",
    "    def cnn_output_size_multilayer(self, n, f, p=0, s=1, num_layers=1):\n",
    "        '''\n",
    "        n: input size\n",
    "        f: kernel size (assuming the same for all layers for simplicity)\n",
    "        p: padding (assuming the same for all layers for simplicity)\n",
    "        s: stride (assuming the same for all layers for simplicity)\n",
    "        num_layers: number of convolutional layers\n",
    "        '''\n",
    "        output_size = n\n",
    "        for _ in range(num_layers):\n",
    "            output_size = int((output_size - f + 2 * p) / s) + 1\n",
    "        return output_size\n",
    "    \n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dense_dropout_rate)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # print('=== before starting layer', x.size())\n",
    "        x = self.starting_layers(x)\n",
    "        # print('=== end of starting layer', x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('=== Flattened', x.size())\n",
    "        \n",
    "        current_num_filters1 = self.cnn_output_size_multilayer(self.in_channel, 12, num_layers=2)\n",
    "        # print(current_num_filters1, current_num_filters2)\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "        # print('=== After Dense layer', x.size())\n",
    "\n",
    "        out = self.out_layer(x)\n",
    "        # print('After out layer', out.size())\n",
    "        return out\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "epoch = 300\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "\n",
    "model = Model(in_channel = 869, \n",
    "              first_h_layer = 469, \n",
    "              out_channel=train_target.shape[1], \n",
    "              num_dense_layers=4,\n",
    "              batch_size=batch_size)\n",
    "\n",
    "model = model.float()\n",
    "model = model.to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_MSE\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"host_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_124384/1304235933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# pred = pred.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mloss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtest_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print('epoch:', e)\n",
    "    for x, y in train_loader:\n",
    "        # print('batch x size:',x.size() )\n",
    "        x_batch = torch.unsqueeze(x, 1).to(device)\n",
    "        y_batch = y.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        # y_batch = y_batch.view(-1)\n",
    "\n",
    "        # y_batch = one_hot_torch(y).to(device)\n",
    "        # print('batch y size before flatten:',y_batch.size())\n",
    "        # y_batch = y_batch.flatten()\n",
    "        # print('batch y size after flatten:',y_batch.size())\n",
    "        # print(x_batch.size())\n",
    "        # print(x_batch.size())\n",
    "# For example, if you have a convolutional layer with 64 output channels, 3 input channels, and a kernel size of 3x3, the weight parameters would have a dimension of (64, 3, 3, 3)\n",
    "        # print(x_batch.size())\n",
    "        pred = model(x_batch.float())\n",
    "        # print(x_batch)\n",
    "        # print(pred)\n",
    "        # pred = pred.unsqueeze(0)\n",
    "        # ic(pred)\n",
    "        # ic(y_batch)\n",
    "        ic(pred.size())\n",
    "        loss_train = criterion(y_batch, pred)\n",
    "        ic(loss_train)\n",
    "        train_batch_loss.append(loss_train.detach())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f'Batch - GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB')\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        # for x, y in test_loader:\n",
    "            # print('x size:',x.size() )\n",
    "        x_batch = torch.unsqueeze(x, 1).to(device)\n",
    "        # print('batch x size:',x_batch.size() )\n",
    "        y_batch = y.to(device)\n",
    "        # print(x_batch.size())\n",
    "        # y_batch = torch.Tensor.float(y).to(device)\n",
    "        # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "        pred = model(x_batch.float())\n",
    "        # pred = pred.unsqueeze(0)\n",
    "\n",
    "        loss_test = criterion(y_batch, pred)\n",
    "        test_batch_loss.append(loss_test )\n",
    "    test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        \n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/rif_model.pt')\n",
    "# fig, ax = plt.subplots()\n",
    "# x = np.arange(1, epoch+1, 1)\n",
    "# ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "# ax.legend()\n",
    "# ax.set_xlabel(\"Number of Epoch\")\n",
    "# ax.set_ylabel(\"Loss\")\n",
    "# ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "# ax.set_title(f'Loss: Learning_rate:{lr}, cnn_dr:{cnn_dr}, cnn_dr:{fc_dr}')\n",
    "# # ax_2 = ax.twinx()\n",
    "# # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# # ax_2.set_yscale(\"log\")\n",
    "# # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "# ax.grid(axis=\"x\")\n",
    "# fig.tight_layout()\n",
    "# fig.show()masked_MSE\n",
    "# fig.savefig(f'./graphs1/loss_lr_{lr}_cnn_dr_{cnn_dr}_fc_dr_{fc_dr}.png')\n",
    "# print(f'./graphs1/loss_lr_{lr}_cnn_dr_{cnn_dr}_fc_dr_{fc_dr}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(file_path, appendix, epoch, lr, dr, train_loss, test_loss):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\">> {appendix}, Epoch: {epoch}, LR: {lr}, DR: {dr}\\n\")\n",
    "        f.write(f\"--- Train Loss: {train_loss}\\n\")\n",
    "        f.write(f\"--- Test Loss: {test_loss}\\n\")\n",
    "        \n",
    "def training(appendix:str, epoch:int, dropout_rate:float, lr:float, batch_size:int=16, train_dataset=train_dataset, val_dataset=val_dataset, verbose:bool=False, graphics:bool=False):\n",
    "    print(f\"====lr: {lr}, dropout rate: {dropout_rate}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "    \n",
    "    model = Model(in_channel = 869, \n",
    "              first_h_layer = 469, \n",
    "              out_channel=1, \n",
    "              num_dense_layers=4,\n",
    "              batch_size=batch_size,\n",
    "              dropout_rate=dropout_rate)\n",
    "    \n",
    "    model = model.float()\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, drop_last=True)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = masked_MSE\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x_batch = torch.unsqueeze(x, 1).to(device)\n",
    "            print(x_batch.size())\n",
    "            # x_batch = x.permute(1, 0, 2)\n",
    "            # print(x_batch.size())\n",
    "            y_batch = y.to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_batch.float()\n",
    "\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            ic(pred.size())\n",
    "            loss_train = criterion(y_batch, pred)\n",
    "            ic(loss_train)\n",
    "            train_batch_loss.append(loss_train.detach())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f'Batch - GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB')\n",
    "\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        with torch.no_grad():\n",
    "            # print('test')\n",
    "            for x, y in test_loader:\n",
    "                x_batch = x.to(device)\n",
    "                y_batch = y.to(device)\n",
    "                pred = model(x_batch.float())\n",
    "                loss_test = criterion(y_batch, pred)\n",
    "                test_batch_loss.append(loss_test )\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if verbose:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "            print('==='*10)\n",
    "        \n",
    "    save_to_file('trials_rif.txt', appendix ,epoch, lr, dropout_rate, train_epoch_loss, test_epoch_loss)\n",
    "\n",
    "    if graphics:\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(1, epoch+1, 1)\n",
    "        ax.plot(x, train_epoch_loss,label='Training')\n",
    "        ax.plot(x, test_epoch_loss,label='Validation')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Number of Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "        ax.set_title(f'Loss: Learning_rate:{lr},dr:{dropout_rate}')\n",
    "        # ax_2 = ax.twinx()\n",
    "        # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "        # ax_2.set_yscale(\"log\")\n",
    "        # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "        ax.grid(axis=\"x\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC/graph_rif/{appendix}_loss_lr_{lr}_dr_{dropout_rate}.png')\n",
    "        fig.show()\n",
    "    return torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy(), torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()\n",
    "# training(epoch=50, dropout_rate=0.2, lr=0.001, batch_size=128, train_dataset=train_dataset, val_dataset=val_dataset, graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====lr: 1e-05, dropout rate: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 869])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'cnn_output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19958/669645224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlr_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdropout_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresult_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'5layers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mresults_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mresults_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19958/2476514937.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(appendix, epoch, dropout_rate, lr, batch_size, train_dataset, val_dataset, verbose, graphics)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19958/2964325757.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# print('=== Flattened', x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mcurrent_num_filters1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mcurrent_num_filters2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_num_filters1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# print(current_num_filters1, current_num_filters2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1208\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'cnn_output_size'"
     ]
    }
   ],
   "source": [
    "lr_values = [1e-5, 1e-3, 1e-1,]\n",
    "# dropout_rates = [0.2, 0.3, 0.4,0.5]\n",
    "dropout_rates = [0]\n",
    "\n",
    "# lr_values = [0.001]\n",
    "# dropout_rates = [0.5]\n",
    "\n",
    "results_test = []\n",
    "results_train = []\n",
    "\n",
    "for lr in lr_values:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        result_test, result_train = training('5layers', epoch=100, dropout_rate=dropout_rate, lr=lr, batch_size=16, train_dataset=train_dataset, val_dataset=val_dataset, graphics=True)\n",
    "        results_test.append((lr, dropout_rate, result_test))\n",
    "        results_train.append((lr, dropout_rate, result_train))\n",
    "# Sort the results based on the validation loss\n",
    "\n",
    "# with open(f'results_test_lr{lr}_dr{dropout_rate}.txt', 'w') as file:\n",
    "#     for lr, dropout_rate, result in results_test:\n",
    "#         file.write(f'Learning Rate: {lr}, Dropout Rate: {dropout_rate}, Test Result: {result}\\n')\n",
    "\n",
    "# # Save the train results to a text file\n",
    "# with open(f'results_train_lr{lr}_dr{dropout_rate}.txt', 'w') as file:\n",
    "#     for lr, dropout_rate, result in results_train:\n",
    "#         file.write(f'Learning Rate: {lr}, Dropout Rate: {dropout_rate}, Train Result: {result}\\n')\n",
    "\n",
    "results_test.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the best lr and dropout rate\n",
    "best_lr, best_dropout_rate, best_result = results_test[0]\n",
    "print(f\"Best lr: {best_lr}, Best dropout rate: {best_dropout_rate}, Best result: {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
