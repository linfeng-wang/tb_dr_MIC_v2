{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbb0ac76970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "def data_prep_(cryptic, gene_list, dr_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "    overlap = overlap['sample_id'].unique()\n",
    "    print(overlap)\n",
    "    print(overlap.shape)\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    crypticSNPnames = np.load('crypticSNPnames.npy', allow_pickle=True)\n",
    "    variants = variants[variants['SNP'].isin(crypticSNPnames)]\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        return output_list\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "    aa = []\n",
    "    dr = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "            if dr_list[0] in variants[variants['sample_id']==x]['drugs'].unique():\n",
    "                dr.append(1)\n",
    "            else:\n",
    "                dr.append(0)\n",
    "        else:\n",
    "            # aa.append([0]*len(all_snp))\n",
    "            pass\n",
    "        # print('SNP')\n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([“ENA_RUN”])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)return aa_array, dr#, mic_aa\n",
    "    return aa_array, dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_drs_cryptic_emb.npy')\n",
    "snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_snps_cryptic_emb.npy')\n",
    "\n",
    "drs = pd.DataFrame(drs)     \n",
    "train_data, test_data, train_target, test_target = data_split(snps, drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110469, 1473)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0),  torch.tensor(mic).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn_mic = get_weighted_masked_cross_entropy_loss(column_weight_maps[0])\n",
    "# weighted_cross_entropy_loss_fn_bi = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_y'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder class\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 100),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, input_size),\n",
    "            nn.Sigmoid()  # Use Sigmoid since input is binary\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# criterion = F.cross_entropy\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.5070, Val Loss: 0.4944\n",
      "Epoch [20/100], Train Loss: 0.5070, Val Loss: 0.4961\n",
      "Epoch [30/100], Train Loss: 0.5070, Val Loss: 0.4909\n",
      "Epoch [40/100], Train Loss: 0.5070, Val Loss: 0.4926\n",
      "Epoch [50/100], Train Loss: 0.5070, Val Loss: 0.5118\n",
      "Epoch [60/100], Train Loss: 0.5070, Val Loss: 0.4874\n",
      "Epoch [70/100], Train Loss: 0.5070, Val Loss: 0.4996\n",
      "Epoch [80/100], Train Loss: 0.5070, Val Loss: 0.4944\n",
      "Epoch [90/100], Train Loss: 0.5070, Val Loss: 0.4891\n",
      "Epoch [100/100], Train Loss: 0.5070, Val Loss: 0.4961\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MLPClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29397/831431837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Initialize and train the MLP classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Predict using the trained MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \"\"\"\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1110\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    894\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. MLPClassifier expected <= 2."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=8)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "\n",
    "for lr in [0.001]:\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=True,\n",
    "    #     with_stack=True\n",
    "    # ) as prof:\n",
    "        # Parameters\n",
    "    input_size = 1473  # Adjust this based on your input vector length\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Instantiate the model, define loss function and optimizer\n",
    "    model = Autoencoder(input_size).to(device)  # Move model to GPU\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for data, _ in train_loader:\n",
    "            # print('new batch', data.shape)\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)  # Move data to GPU\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data)\n",
    "                val_loss.append(loss.item())\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}')\n",
    "\n",
    "# prof.export_chrome_trace(\"torch_trace.json\")\n",
    "\n",
    "# Plot train loss and val loss\n",
    "\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "    print('Training complete.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n",
    "\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29397/4058002314.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.48%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      1891\n",
      "           1       0.24      0.07      0.11       564\n",
      "\n",
      "    accuracy                           0.73      2455\n",
      "   macro avg       0.51      0.50      0.48      2455\n",
      "weighted avg       0.65      0.73      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 96.82%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1891\n",
      "           1       0.94      0.92      0.93       564\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.96      0.95      0.95      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "    model.eval()\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2455,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.66%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.87      1879\n",
      "           1       0.55      0.03      0.06       576\n",
      "\n",
      "    accuracy                           0.77      2455\n",
      "   macro avg       0.66      0.51      0.46      2455\n",
      "weighted avg       0.72      0.77      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 97.27%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1879\n",
      "           1       0.95      0.93      0.94       576\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.97      0.96      0.96      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performan\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autonencoder_model_state.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(variants['sample_id'].unique()):\n",
    "        aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "\n",
    "    return aa_array, mic_aa\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array#, mic_aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
