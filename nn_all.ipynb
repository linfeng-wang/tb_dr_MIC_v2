{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# # train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# # test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB_MIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8031</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10161</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9156 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       EMB_MIC\n",
       "5496         1\n",
       "6066         0\n",
       "324          0\n",
       "2668         0\n",
       "8031         0\n",
       "...        ...\n",
       "155          0\n",
       "10161        0\n",
       "785          0\n",
       "5806         0\n",
       "3229         0\n",
       "\n",
       "[9156 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(train_target):\n",
    "    column_weight_maps = {}\n",
    "    y_true = train_target\n",
    "    # y_true = pd.concat([train_target, test_target])\n",
    "    for column in y_true.columns:\n",
    "        column_values = y_true[column].dropna().values\n",
    "        values, counts = np.unique(column_values, return_counts=True)\n",
    "        frequency = counts / len(column_values)\n",
    "        \n",
    "        # Calculate weights as the inverse of frequencies\n",
    "        weights_inverse = 1/frequency\n",
    "        # weights_inverse = 1 - frequency\n",
    "        \n",
    "        # Normalize weights to ensure they sum up to 1\n",
    "        weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "        \n",
    "        # Map each MIC value to its corresponding weight\n",
    "        weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "        \n",
    "        column_weight_maps[column] = weight_map\n",
    "    # return column_weight_maps[0]\n",
    "    return list(column_weight_maps.items())[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = pd.read_csv('variants_full.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CRyPTIC_reuse_table_20231208.csv')\n",
    "gene_list = variants['gene'].unique()\n",
    "\n",
    "\n",
    "# Define the array\n",
    "aa_array, mic_aa = data_prep_(df, gene_list)\n",
    "mic_aa = mic_aa[['AMI_MIC', 'BDQ_MIC','CFZ_MIC', 'DLM_MIC', 'EMB_MIC', 'ETH_MIC', 'INH_MIC', 'KAN_MIC','LEV_MIC', 'LZD_MIC', 'MXF_MIC', 'RIF_MIC', 'RFB_MIC']]\n",
    "mic_aa = mic_aa.dropna().values\n",
    "\n",
    "# Iterate over the array\n",
    "for i in range(len(mic_aa)):\n",
    "    for y in range(len(mic_aa[i])):\n",
    "        if '<=' in mic_aa[i][y]:\n",
    "            # Get the number following '<'\n",
    "            num = float(mic_aa[i][y].split('<=')[1])\n",
    "            # Update the value to half of the number following it\n",
    "            mic_aa[i][y] = mic_aa[i][y].split('<')[0] + str(num / 2)\n",
    "        elif '>=' in mic_aa[i][y]:\n",
    "            # Get the number over '>'\n",
    "            num = float(mic_aa[i][y].split('>=')[1])\n",
    "            # Update the value to twice the size of the number over it\n",
    "            mic_aa[i][y] = mic_aa[i][y].split('>')[0] + str(num * 2)\n",
    "        elif '<' in mic_aa[i][y]:\n",
    "            # Get the number following '<'\n",
    "            num = float(mic_aa[i][y].split('<')[1])\n",
    "            # Update the value to half of the number following it\n",
    "            mic_aa[i][y] = mic_aa[i][y].split('<')[0] + str(num / 2)\n",
    "        elif '>' in mic_aa[i][y]:\n",
    "            # Get the number over '>'\n",
    "            num = float(mic_aa[i][y].split('>')[1])\n",
    "            # Update the value to twice the size of the number over it\n",
    "            mic_aa[i][y] = mic_aa[i][y].split('>')[0] + str(num * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_aa = mic_aa.astype('float')\n",
    "mic_series = np.log2(mic_aa)\n",
    "mic_series = mic_series.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10511, 13)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -4, -3, -6,  1,  4, -1,  2,  0,  0, -1, -3, -5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = np.min(mic_series, axis=0)  # Calculate the minimum values for each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for column '0':\n",
      "0\n",
      "-3    6205\n",
      "-1    3030\n",
      " 0     521\n",
      " 4     384\n",
      " 5     240\n",
      " 1      77\n",
      " 2      28\n",
      " 3      26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '1':\n",
      "1\n",
      "-5    3674\n",
      "-4    3364\n",
      "-7    1641\n",
      "-6     867\n",
      "-3     689\n",
      "-2     192\n",
      "-1      48\n",
      " 0      24\n",
      " 1       7\n",
      " 2       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '2':\n",
      "2\n",
      "-5    3579\n",
      "-3    2412\n",
      "-4    1856\n",
      "-6    1154\n",
      "-2    1041\n",
      "-1     303\n",
      " 0     111\n",
      " 1      34\n",
      " 2      13\n",
      " 3       8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '3':\n",
      "3\n",
      "-7    6792\n",
      "-6    1562\n",
      "-5    1204\n",
      "-4     581\n",
      "-3     215\n",
      "-2      51\n",
      " 1      42\n",
      " 0      34\n",
      "-1      30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '4':\n",
      "4\n",
      " 0    3472\n",
      " 1    2638\n",
      " 2    1340\n",
      " 3    1231\n",
      "-1     992\n",
      " 4     614\n",
      "-2      82\n",
      " 5      61\n",
      "-3      52\n",
      "-5      17\n",
      " 6      12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '5':\n",
      "5\n",
      " 0    3938\n",
      " 1    2490\n",
      "-1    1647\n",
      " 2     803\n",
      " 4     782\n",
      " 3     633\n",
      "-3     218\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '6':\n",
      "6\n",
      "-4    3010\n",
      " 1    2251\n",
      "-6    2229\n",
      " 0    1538\n",
      "-2     305\n",
      " 2     300\n",
      "-1     259\n",
      "-3     242\n",
      " 4     206\n",
      " 3     171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '7':\n",
      "7\n",
      " 1    5055\n",
      "-1    3112\n",
      " 2    1410\n",
      " 5     631\n",
      " 3     216\n",
      " 4      87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '8':\n",
      "8\n",
      "-1    4946\n",
      "-2    2706\n",
      " 0     701\n",
      " 2     591\n",
      " 3     579\n",
      "-4     376\n",
      " 1     362\n",
      " 4     250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '9':\n",
      "9\n",
      "-1    5189\n",
      "-2    2842\n",
      " 0    1400\n",
      "-3     691\n",
      "-4     152\n",
      " 1      64\n",
      "-6      62\n",
      " 2      56\n",
      "-5      44\n",
      " 3      11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '10':\n",
      "10\n",
      "-2    3535\n",
      "-1    2314\n",
      "-3    1899\n",
      "-5     840\n",
      " 2     517\n",
      " 0     499\n",
      " 3     481\n",
      " 1     426\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '11':\n",
      "11\n",
      " 3    2194\n",
      "-5    2077\n",
      "-3    1830\n",
      " 4    1438\n",
      "-4    1159\n",
      "-6     658\n",
      "-2     630\n",
      "-1     157\n",
      " 2     139\n",
      " 1     126\n",
      " 0     103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for column '12':\n",
      "12\n",
      "-5    6356\n",
      " 2    2589\n",
      " 1     442\n",
      "-3     409\n",
      " 0     293\n",
      "-2     243\n",
      "-1     179\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_array' is the name of your data array\n",
    "df = pd.DataFrame(mic_series)\n",
    "\n",
    "# Print value counts for each column\n",
    "for column in df.columns:\n",
    "    print(f\"Value counts for column '{column}':\")\n",
    "    print(df[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -7, -6, -7, -5, -3, -6, -1, -4, -6, -5, -6, -5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your array is named 'arr'\n",
    "min_values = np.min(mic_aa, axis=0)  # Calculate the minimum values for each column\n",
    "abs_min_values = np.abs(min_values)  # Calculate the absolute values of the minimum values\n",
    "result = arr + abs_min_values  # Add the absolute minimum values to each value in the array\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63755/393090413.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb.loc[i, f'{x}'] = '16'\n",
      "/tmp/ipykernel_63755/393090413.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float')\n",
      "100%|██████████| 11310/11310 [01:11<00:00, 158.84it/s]\n",
      "/tmp/ipykernel_63755/3526634147.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
      "/tmp/ipykernel_63755/3526634147.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n"
     ]
    }
   ],
   "source": [
    "column_weight_maps = {}\n",
    "column_minmax_maps = {}\n",
    "\n",
    "df = pd.read_csv('CRyPTIC_reuse_table_20231208.csv')\n",
    "gene_list = variants['gene'].unique()\n",
    "df_emb = df[df['EMB_MIC'].isin(['>8','8.0', '4.0', '2.0', '1.0', '0.5'])]\n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "for i, row in df_emb.iterrows():\n",
    "    x = 'EMB_MIC'\n",
    "    if row[x] == '>8' :\n",
    "        df_emb.loc[i, f'{x}'] = '16'\n",
    "    elif row[x] == '<=0.25':\n",
    "        df_emb.loc[i, f'{x}'] = '0.125'\n",
    "        \n",
    "df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float') \n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "\n",
    "# variants = pd.read_csv('variants_full.csv')\n",
    "# variants = variants[variants['type'] != 'synonymous_variant']\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['LOW','MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[df_emb['ENA_RUN'].isin(samples)]\n",
    "cryptic = df_emb\n",
    "aa_array, mic_aa = data_prep(cryptic, gene_list)\n",
    "# encoded_mic = np.array([0 if value < 4 else 1 for value in mic_aa['EMB_MIC'].to_list()])\n",
    "\n",
    "encoded_mic = mic_aa['EMB_MIC'].to_list()\n",
    "\n",
    "# train_data, test_data, train_target, test_target  = data_split(aa_array, encoded_mic)\n",
    "\n",
    "mic_series = np.log2(mic_aa['EMB_MIC'])\n",
    "mic_series += 1\n",
    "sample_ids = mic_aa['ENA_RUN']\n",
    "\n",
    "train_data, test_data, train_target, test_target = data_split(aa_array, mic_series)\n",
    "train_target = train_target.to_frame()\n",
    "test_target = test_target.to_frame()\n",
    "target_min, target_max = mic_series.min(), mic_series.max()\n",
    "\n",
    "weight_column = get_weights(train_target)\n",
    "\n",
    "column_weight_maps[weight_column[0]] = weight_column[1]\n",
    "\n",
    "##########################\n",
    "drug = 'ETH'\n",
    "gene_list = ['ethA', 'inhA']\n",
    "df = pd.read_csv('CRyPTIC_reuse_table_20231208.csv')\n",
    "# df_emb = df[df[f'{drug}_MIC'].isin(['>8','8.0', '4.0', '2.0', '1.0', '0.5', '0.25', '<=0.25'])]\n",
    "df_emb = df[df[f'{drug}_MIC'].isin(['>8','8.0', '4.0', '2.0', '1.0', '0.5'])]\n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "for i, row in df_emb.iterrows():\n",
    "    x = f'{drug}_MIC'\n",
    "    if row[x] == '>8' :\n",
    "        df_emb.loc[i, f'{x}'] = '16'\n",
    "    elif row[x] == '<=0.25':\n",
    "        df_emb.loc[i, f'{x}'] = '0.25'\n",
    "        \n",
    "df_emb[f'{drug}_MIC'] = df_emb[f'{drug}_MIC'].astype('float') \n",
    "\n",
    "cryptic = df_emb\n",
    "aa_array, mic_aa = data_prep(cryptic, gene_list)\n",
    "# encoded_mic = np.array([0 if value < 4 else 1 for value in mic_aa['{drug}_MIC'].to_list()])\n",
    "\n",
    "encoded_mic = mic_aa[f'{drug}_MIC'].to_list()\n",
    "\n",
    "# train_data, test_data, train_target, test_target  = data_split(aa_array, encoded_mic)\n",
    "mic_series = np.log2(mic_aa[f'{drug}_MIC'])\n",
    "sample_ids = mic_aa['ENA_RUN']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cornloss weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = torch.from_numpy(train_target.values).flatten()\n",
    "# train_target_counts = torch.tensor([0,1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def compute_class_weights(y_train, num_classes):\n",
    "#     class_counts = torch.bincount(y_train, minlength=num_classes)\n",
    "#     print(class_counts)\n",
    "#     class_weights = 1.0 / class_counts.float()\n",
    "#     class_weights = class_weights / class_weights.sum() * num_classes  # Normalize weights\n",
    "#     return class_weights\n",
    "\n",
    "# # Example usage\n",
    "# num_classes = 3\n",
    "# class_weights = compute_class_weights(train_target_counts, num_classes)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_corn(logits, y_train, num_classes, class_weights):\n",
    "#     sets = []\n",
    "#     for i in range(num_classes - 1):\n",
    "#         label_mask = y_train > i - 1\n",
    "#         label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "#         sets.append((label_mask, label_tensor))\n",
    "\n",
    "#     num_examples = 0\n",
    "#     losses = 0.\n",
    "#     for task_index, s in enumerate(sets):\n",
    "#         train_examples = s[0]\n",
    "#         train_labels = s[1]\n",
    "\n",
    "#         if len(train_labels) < 1:\n",
    "#             continue\n",
    "\n",
    "#         num_examples += len(train_labels)\n",
    "#         pred = logits[train_examples, task_index]\n",
    "\n",
    "#         # Apply class weights\n",
    "#         weights = class_weights[train_labels].to(logits.device)\n",
    "        \n",
    "#         loss = -torch.sum(weights * (F.logsigmoid(pred) * train_labels +\n",
    "#                                      (F.logsigmoid(pred) - pred) * (1 - train_labels)))\n",
    "#         losses += loss\n",
    "#     return losses / num_examples\n",
    "\n",
    "# # Example usage\n",
    "# # logits = torch.randn(9, 2).cuda()  # Example logits tensor\n",
    "# # y_train = torch.tensor([0, 1, 2, 0, 1, 2, 2, 1, 0]).cuda()  # Example training labels\n",
    "# # class_weights = compute_class_weights(y_train.cpu(), num_classes).cuda()\n",
    "\n",
    "# # loss = loss_corn(logits, y_train, num_classes, class_weights)\n",
    "# # print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "# Julian's code - implement this, might be faster\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            res = self.res_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            res = self.res_df.loc[index]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "# y_true = train_target\n",
    "# # y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "# column_weight_maps = {}\n",
    "\n",
    "# for column in y_true.columns:\n",
    "#     column_values = y_true[column].dropna().values\n",
    "#     values, counts = np.unique(column_values, return_counts=True)\n",
    "#     frequency = counts / len(column_values)\n",
    "    \n",
    "#     # Calculate weights as the inverse of frequencies\n",
    "#     weights_inverse = 1/frequency\n",
    "#     # weights_inverse = 1 - frequency\n",
    "    \n",
    "#     # Normalize weights to ensure they sum up to 1\n",
    "#     weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "#     # Map each MIC value to its corresponding weight\n",
    "#     weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "#     column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB_MIC': {0.0: 0.1772350639643713,\n",
       "  1.0: 0.05011749905730602,\n",
       "  2.0: 0.06728492398041802,\n",
       "  3.0: 0.13236542751769503,\n",
       "  4.0: 0.144643424862705,\n",
       "  5.0: 0.4283536606175047}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_weight_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,  # New parameter\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength = 0.1,\n",
    "        return_logits=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "        \n",
    "        # now define the actual model\n",
    "        # self.feature_extraction_layer = self._conv_layer(\n",
    "            # in_channels, num_filters, filter_length\n",
    "        # )\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "        #dynamic filter scaling from deepram\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters1 * filter_scaling_factor), 3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "            \n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters2 * filter_scaling_factor), 3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "            \n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [53568]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        )\n",
    "        \n",
    "        # self.dense_layers = nn.ModuleList(\n",
    "            # self._dense_layer(input_dim, num_dense_neurons)\n",
    "            # for input_dim in [current_num_filters2]\n",
    "            # + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        # )\n",
    "        \n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(num_dense_neurons, num_classes)\n",
    "            if return_logits\n",
    "            else nn.Sequential(nn.Linear(num_dense_neurons, num_classes), nn.ReLU()) #difference between sequential and nn.moduleList?\n",
    "        )\n",
    "        \n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        self.apply(self.init_weights)    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def l1_regularization(self):\n",
    "        l1_loss_example = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss_example += torch.sum(torch.abs(param))\n",
    "        return self.l1_strength * l1_loss_example\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first pass over input\n",
    "        # print(x.size())\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        x = self.feature_extraction_layer(x)\n",
    "        # print(\"After feature extraction shape:\", x.shape)\n",
    "\n",
    "        # conv layers\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        # global max pool 1D\n",
    "        x = self.m(x)\n",
    "        # print(x.shape)\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        \n",
    "        # x = torch.max(x, dim=-1).values\n",
    "        x = x.view(x.size(0), -1)  # Flattening the tensor to [batch_size, features]\n",
    "        # ic(x.shape)\n",
    "        # fully connected layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "        x = self.prediction_layer(x)\n",
    "        return x\n",
    "\n",
    "# def l1loss(layer): # https://stackoverflow.com/questions/50054049/lack-of-sparse-solution-with-l1-regularization-in-pytorch\n",
    "#     return torch.norm(layer.weight, p=1)\n",
    "\n",
    "# def l1loss(sequence):\n",
    "#     l1_regularization = 0\n",
    "#     for module in sequence.modules():\n",
    "#         if isinstance(module, nn.Conv1d):  # Check if the module is a Conv1d layer\n",
    "#             l1_regularization += torch.norm(module.weight, p=1)\n",
    "#     return l1_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "# conv_dropout_rate=conv_dropout_rate,\n",
    "# dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51147/3156842831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_51147/3771095047.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# print(\"Input shape:\", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# print(\"After feature extraction shape:\", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    301\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 303\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    304\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.0\n",
    "dense_dropout_rate=0.0\n",
    "weight_decay=1e-8\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=1,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "        # print(pred)\n",
    "        # print(y_batch)\n",
    "        # print(loss_train)\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "        # break\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e%29 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "lr: 1e-06 epoch: 400 conv_dropout_rate: 0 dense_dropout_rate: 0.1 weight_decay: 0\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [02:33<18:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.14512404799461365\n",
      "Validation loss: 0.14181280136108398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [05:06<15:05,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.13658545911312103\n",
      "Validation loss: 0.1411546915769577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 147/400 [07:32<13:02,  3.09s/it]"
     ]
    }
   ],
   "source": [
    "for lr in [1e-6, 1e-4]:\n",
    "    for dr in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "\n",
    "        #input parameter\n",
    "        lr = lr\n",
    "        # lr = 0.0001\n",
    "        epoch = 400\n",
    "        conv_dropout_rate=0\n",
    "        dense_dropout_rate=dr\n",
    "        weight_decay=0\n",
    "        \n",
    "        print(\"=\"*20)\n",
    "        print(\"lr:\", lr, \"epoch:\", epoch, \"conv_dropout_rate:\", conv_dropout_rate, \"dense_dropout_rate:\", dense_dropout_rate, \"weight_decay:\", weight_decay)\n",
    "        print(\"=\"*20)\n",
    "        ######################################\n",
    "\n",
    "        model = Model(\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        num_conv_layers=2,\n",
    "        # num_dense_neurons=256, # batch_size = 64\n",
    "        num_dense_neurons=128, # batch_size = 64\n",
    "        num_dense_layers=2,\n",
    "        return_logits=False,\n",
    "        conv_dropout_rate=conv_dropout_rate,\n",
    "        dense_dropout_rate=dense_dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "        # model = Model( #! way too memory intensive\n",
    "        # num_classes=13,\n",
    "        # num_filters=128,\n",
    "        # num_conv_layers=2,\n",
    "        # num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "        # num_dense_layers=2,\n",
    "        # return_logits=True,\n",
    "        # conv_dropout_rate=0,\n",
    "        # dense_dropout_rate=0\n",
    "        # ).to(device)\n",
    "        ## early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "        patience_counter = 0\n",
    "        lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "        batch_size = 128\n",
    "        # lr = 0.0085\n",
    "        # lr = 0.00002\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "        test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "        # train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "        # test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "        # criterion = nn.MSELoss()\n",
    "        # criterion = masked_weighted_MAE\n",
    "        # criterion = masked_weighted_MSE\n",
    "        criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "        # criterion = masked_MAE\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "        # scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "        #%%\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc; gc.collect()\n",
    "        # ic.enable()\n",
    "        ic.disable()\n",
    "\n",
    "        train_epoch_loss = []\n",
    "        test_epoch_loss = []\n",
    "\n",
    "        for e in tqdm(range(1, epoch+1)):\n",
    "            model.train()\n",
    "            train_batch_loss = []\n",
    "            test_batch_loss = []\n",
    "            # print(f'Epoch {e}')\n",
    "            for x_train, y_train in train_loader:\n",
    "                x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "                y_batch = y_train.to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                pred = model(x_batch.float())\n",
    "\n",
    "                # break\n",
    "                loss_train = criterion(pred,y_batch)\n",
    "                train_batch_loss.append(loss_train)        \n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "                # scheduler.step()  # Update the learning rate\n",
    "\n",
    "            train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # print('>> test')\n",
    "                for x_test, y_test in test_loader:\n",
    "                    x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                    x_batch = x_batch.float()\n",
    "                    y_batch = y_test.to(device)\n",
    "                    # print(x_batch.size())\n",
    "                    # y_batch = torch.Tensor.float(y).to(device)\n",
    "                    # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "                    pred = model(x_batch.float())\n",
    "\n",
    "                    # pred = pred.unsqueeze(0)\n",
    "                    # print(pred[:10])\n",
    "                    # print(y_batch[:10])\n",
    "\n",
    "                    loss_test = criterion(pred,y_batch)\n",
    "                    test_batch_loss.append(loss_test)\n",
    "                test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "            if e % 50 == 0:\n",
    "                print(f'Epoch {e}')\n",
    "                print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "                print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "            # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "            # print(train_batch_loss)\n",
    "            # print(test_batch_loss)\n",
    "            # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "            # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "            # #! implementing early stopping\n",
    "            # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "            # print(f'Current val loss: {current_val_loss}')\n",
    "            # print(f'Best val loss: {best_val_loss}')\n",
    "            # if current_val_loss < best_val_loss:\n",
    "            #     best_val_loss = current_val_loss\n",
    "            #     patience_counter = 0  # reset patience counter\n",
    "            #     # Save the best model\n",
    "            #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "            # else:\n",
    "            #     patience_counter += 1\n",
    "            #     if patience_counter >= patience:\n",
    "            #         print(\"Early stopping triggered\")\n",
    "            #         torch.save({\n",
    "            #         'optimizer': optimizer.state_dict(),\n",
    "            #         'model': model.state_dict(),\n",
    "            #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "            #         break  # Early stopping\n",
    "                \n",
    "        print('==='*10)\n",
    "        # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "        save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                    train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(1, epoch+1, 1)\n",
    "        ax.plot(x, train_epoch_loss,label='Training')\n",
    "        ax.plot(x, test_epoch_loss,label='Validation')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Number of Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "        ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "        # ax_2 = ax.twinx()\n",
    "        # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "        # ax_2.set_yscale(\"log\")\n",
    "        # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "        ax.grid(axis=\"x\")\n",
    "        fig.tight_layout()\n",
    "        fig.show()\n",
    "        fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "        print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "        #%%\n",
    "        testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "        testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "        model.eval()  # For inference\n",
    "\n",
    "        ic.disable()\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        target_list  = []\n",
    "        mse_list = []\n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in testing_loader1:\n",
    "                xtest1 = x_test.to(device).float()\n",
    "                ytest1 = y_test.to(device).float()\n",
    "                pred = model(xtest1)\n",
    "                pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "                target_list.append(y_test.detach().cpu().numpy())\n",
    "        target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "        def calculate_metrics(true_labels, predictions):\n",
    "            \"\"\"\n",
    "            Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "            Parameters:\n",
    "            - true_labels: List or array of true labels\n",
    "            - predictions: List or array of predicted labels\n",
    "\n",
    "            Returns:\n",
    "            - accuracy: Overall accuracy of predictions\n",
    "            - f1: Weighted average F1 score\n",
    "            - conf_matrix: Multiclass confusion matrix\n",
    "            - mae: Mean Absolute Error of predictions\n",
    "            \"\"\"\n",
    "            # Ensure inputs are numpy arrays for consistency\n",
    "            true_labels = np.array(true_labels)\n",
    "            predictions = np.array(predictions)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "            # Calculate F1 score\n",
    "            f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "            # Calculate MAE\n",
    "            mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "            return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "        # Example usage\n",
    "        # true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "        # predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "        accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "        print(\"======================\")\n",
    "        print(\"Model's Named Parameters:\")\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(f\"Name: {name}\")\n",
    "        #     print(f\"Shape: {param.size()}\")\n",
    "        #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "        #     print('-----')\n",
    "        print(\"Optimizer details:\")\n",
    "        print(optimizer)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"Learning rate:\", param_group['lr'])\n",
    "            print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "            \n",
    "        print(\"======================\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Mae: {mae}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"conf_matrix: {conf_matrix}\")\n",
    "        print(\"======================\")\n",
    "        doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "        print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cycle lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0\n",
    "dense_dropout_rate=0\n",
    "# weight_decay=1e-8\n",
    "weight_decay=0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epoch)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if ep % 20 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.05\n",
    "dense_dropout_rate=0.5\n",
    "weight_decay=1e-8\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-9, max_lr=0.001)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=3,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "        train_batch_loss.append(loss_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes-1):\n",
    "        label_mask = y_train > i-1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(F.logsigmoid(pred)*train_labels\n",
    "                          + (F.logsigmoid(pred) - pred)*(1-train_labels)\n",
    "                          )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = torch.tensor([[0.0000, 0.5111],\n",
    "        [0.1329, 1.1051]], device='cuda:0')\n",
    "target = torch.tensor([0, 0], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7275, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = loss_corn(pred, target, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
