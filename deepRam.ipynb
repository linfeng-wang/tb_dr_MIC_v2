{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88002a5df0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from extract_motifs import get_motif\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "#%%\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/gene_seq_train.csv')\n",
    "train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/res_train.csv')\n",
    "#don't touch test data, split out validation data from training data during training\n",
    "test_data = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/gene_seq_test.csv')\n",
    "test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC/data/res_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chip():\n",
    "    def __init__(self,filename,motiflen=24):\n",
    "        self.file = filename\n",
    "        self.motiflen = motiflen\n",
    "  \n",
    "    def openFile(self):\n",
    "        train_dataset=[]\n",
    "        sequences=[]\n",
    "        with gzip.open(self.file, 'rt') as data:\n",
    "                next(data)\n",
    "                reader = csv.reader(data,delimiter='\\t')\n",
    "\n",
    "                if embedding:\n",
    "\n",
    "                        for row in reader:\n",
    "\n",
    "                            ## When using Embedding\n",
    "                            sequences.append(row[0])\n",
    "                            \n",
    "                            train_dataset.append([row[0],[int(row[1])]])\n",
    "                else:\n",
    "                        for row in reader:\n",
    "                                \n",
    "                                train_dataset.append([seqtopad(row[0],self.motiflen),[int(row[1])]])\n",
    "  \n",
    "        random.shuffle(train_dataset)\n",
    "        size=int(len(train_dataset)/3)\n",
    "        firstvalid=train_dataset[:size]\n",
    "        secondvalid=train_dataset[size:size+size]\n",
    "        thirdvalid=train_dataset[size+size:]\n",
    "        firsttrain=secondvalid+thirdvalid\n",
    "        secondtrain=firstvalid+thirdvalid\n",
    "        thirdtrain=firstvalid+secondvalid\n",
    "        return firsttrain,firstvalid,secondtrain,secondvalid,thirdtrain,thirdvalid,train_dataset,sequences\n",
    "        \n",
    "def Gen_Words(sequences,kmer_len,s): # this function is used to generate kmer list from sequences\n",
    "    out=[]\n",
    "\n",
    "    for i in sequences:\n",
    "\n",
    "        kmer_list=[]\n",
    "        for j in range(0,(len(i)-kmer_len)+1,s):\n",
    "\n",
    "              kmer_list.append(i[j:j+kmer_len])\n",
    "\n",
    "        out.append(kmer_list)\n",
    "\n",
    "    return out\n",
    "\n",
    "class chipseq_dataset(Dataset): # this class is used to generate dataset for training\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,xy=None):\n",
    "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class chipseq_dataset_embd(Dataset):  # this class is used to generate dataset for training with embedding\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,xy=None,model=None,kmer_len=5,stride=2):\n",
    "      \n",
    "        self.kmer_len= kmer_len\n",
    "        self.stride= stride\n",
    "        data=[el[0] for el in xy]\n",
    "        words_doc= self.Gen_Words(data,self.kmer_len,self.stride)\n",
    "#         print(words_doc[0])\n",
    "        x_data=[self.convert_data_to_index(el,model.wv) for el in words_doc]\n",
    "#         print(x_data[0])\n",
    "       \n",
    "        \n",
    "        self.x_data=np.asarray(x_data,dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.LongTensor(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "      \n",
    "    def Gen_Words(self,pos_data,kmer_len,s):\n",
    "        out=[]\n",
    "        \n",
    "        for i in pos_data:\n",
    "\n",
    "            kmer_list=[]\n",
    "            for j in range(0,(len(i)-kmer_len)+1,s):\n",
    "\n",
    "                  kmer_list.append(i[j:j+kmer_len])\n",
    "                \n",
    "            out.append(kmer_list)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "    def convert_data_to_index(self, string_data, wv): # this function is used to convert kmer list to index list\n",
    "      index_data = []\n",
    "      for word in string_data:\n",
    "          if word in wv:\n",
    "              index_data.append(wv.vocab[word].index)\n",
    "      return index_data\n",
    "      \n",
    "      \n",
    "class Chip_test(): # this class is used to generate test dataset\n",
    "    def __init__(self,filename,motiflen=24):\n",
    "        self.file = filename\n",
    "        self.motiflen = motiflen\n",
    "    def openFile(self):\n",
    "        test_dataset=[]\n",
    "        seq=[]\n",
    "\n",
    "\n",
    "        with gzip.open(self.file, 'rt') as data:\n",
    "            next(data)\n",
    "            reader = csv.reader(data,delimiter='\\t')\n",
    "            if embedding:\n",
    "                if evaluate_performance: \n",
    "                    for row in reader:\n",
    "\t\t\t\t\t\t## When using Embedding\n",
    "                        test_dataset.append([row[0],[int(row[1])]])\n",
    "                        seq.append(row[0])\n",
    "                else:\n",
    "                    for row in reader:\n",
    "\t\t\t\t\t\t## just adding fake label but it will not be used\n",
    "                        test_dataset.append([row[0],[1]])\n",
    "                        seq.append(row[0])\n",
    "\t\t\t\t\t\t\n",
    "            else:\n",
    "                if evaluate_performance: \n",
    "                    for row in reader:\n",
    "                        test_dataset.append([seqtopad(row[0],self.motiflen),[int(row[1])]])\n",
    "                        seq.append(row[0])\n",
    "                else:\n",
    "                    for row in reader:\n",
    "\t\t\t\t\t\t## just adding fake label but it will not be used\n",
    "                        test_dataset.append([seqtopad(row[0],self.motiflen),[1]])\n",
    "                        seq.append(row[0])\n",
    "        return test_dataset,seq    \n",
    "\n",
    "\n",
    "\n",
    "train_dataloader=[]\n",
    "valid_dataloader=[]\n",
    "test_loader=[]\n",
    "sequences=[]\n",
    "seq=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.2913e-01,  2.7619e-01, -5.3888e-01,  4.6258e-01, -8.7189e-01,\n",
       "          -2.7118e-02],\n",
       "         [-3.5325e-01,  1.4639e+00,  5.6351e-01,  1.8582e+00,  1.0441e+00,\n",
       "          -8.6382e-01],\n",
       "         [ 8.3509e-01, -3.1571e-01,  2.6911e-01,  8.5404e-02, -1.4129e+00,\n",
       "          -1.8791e+00],\n",
       "         [-1.7983e-01,  7.9039e-01,  5.2394e-01, -2.6935e-01, -1.6191e+00,\n",
       "           1.2588e-03]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randn(nummotif,self.input_channels,motiflen)\n",
    "torch.randn(1,4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, nummotif,motiflen,RNN_hidden_size,hidden_size,hidden,dropprob,sigmaConv,sigmaNeu,sigmaRNN,xavier_init):\n",
    "      \n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.hidden=hidden\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        \n",
    "        self.dropprob=dropprob\n",
    "        self.sigmaConv=sigmaConv\n",
    "        self.sigmaNeu=sigmaNeu\n",
    "        self.hidden_size=hidden_size  \n",
    "        self.input_channels=4\n",
    "        \n",
    "        # Embedding\n",
    "        if embedding:\n",
    "              model1 = gensim.models.Word2Vec.load(word2vec_model)\n",
    "              weights = torch.FloatTensor(model1.wv.vectors)\n",
    "              self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "              self.input_channels=Embsize\n",
    "              \n",
    "        # Convnet\n",
    "        self.ConvWeights=[]\n",
    "        self.ConvBias=[]\n",
    "        self.wConv=torch.randn(nummotif,self.input_channels,motiflen).to(device)\n",
    "        self.wRect=torch.randn(nummotif).to(device)\n",
    "        self.ConvWeights.append(self.wConv)\n",
    "        self.ConvBias.append(self.wRect)\n",
    "        conv_channels=nummotif\n",
    "        \n",
    "        if conv:\n",
    "              self.FC_size= nummotif\n",
    "              self.input_channels=nummotif\n",
    "              for c in range(1,conv_layers):\n",
    "                  Wconv=torch.randn(int(1.5*c*nummotif),conv_channels,motiflen).to(device)\n",
    "                  Bconv=torch.randn(int(1.5*c*nummotif)).to(device)\n",
    "                  self.ConvWeights.append(Wconv)\n",
    "                  self.ConvBias.append(Bconv)\n",
    "                  conv_channels=int(1.5*c*nummotif) # number of channels in the next layer\n",
    "                  self.FC_size= int(1.5*c*nummotif)\n",
    "                  self.input_channels=int(1.5*c*nummotif)\n",
    "              torch.nn.init.normal_(self.ConvWeights[0],mean=0,std=sigmaConv)\n",
    "              \n",
    "              ind=0\n",
    "              for weights in self.ConvWeights:\n",
    "                  weights.requires_grad=True\n",
    "                  if ind>0:\n",
    "                    if dilation>1:\n",
    "                      torch.nn.init.normal_(weights,mean=0,std=0.1)\n",
    "                      print('ffffffff')\n",
    "                    else:\n",
    "                      torch.nn.init.xavier_uniform(weights)\n",
    "                  ind=ind+1\n",
    "              for weights in self.ConvBias:\n",
    "                  weights.requires_grad=True\n",
    "                  torch.nn.init.normal_(weights)\n",
    "            \n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "        if RNN:\n",
    "              if RNN_type=='GRU':\n",
    "                  self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=False).to(device)\n",
    "                  self.FC_size= RNN_hidden_size\n",
    "              elif RNN_type=='BiGRU':\n",
    "                  self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=True).to(device)\n",
    "                  self.FC_size= 2*RNN_hidden_size\n",
    "              elif RNN_type=='LSTM':\n",
    "                  self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=False).to(device)\n",
    "                  self.FC_size= RNN_hidden_size\n",
    "              elif RNN_type=='BiLSTM':\n",
    "                  self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=RNN_layers, bidirectional=True).to(device)\n",
    "                  self.FC_size= 2*RNN_hidden_size\n",
    "              if not xavier_init:\n",
    "                for layer_p in self.rnn._all_weights:\n",
    "                  for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                      torch.nn.init.normal_(self.rnn.__getattr__(p),mean=0,std=sigmaRNN)\n",
    "              else:\n",
    "                for layer_p in self.rnn._all_weights:\n",
    "                  for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                      torch.nn.init.xavier_uniform(self.rnn.__getattr__(p))\n",
    "\n",
    "        # FC    \n",
    "        self.wHidden=torch.randn(self.FC_size,self.hidden_size).to(device)\n",
    "        self.wHiddenBias=torch.randn(self.hidden_size).to(device)\n",
    "        self.wHidden.requires_grad=True\n",
    "        self.wHiddenBias.requires_grad=True\n",
    "        \n",
    "        if not self.hidden:\n",
    "            self.wNeu=torch.randn(self.FC_size,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            if not xavier_init:\n",
    "              torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "            else:\n",
    "              torch.nn.init.xavier_uniform(self.wNeu)\n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "           \n",
    "            self.wNeu=torch.randn(self.hidden_size,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)     \n",
    "            if not xavier_init:  \n",
    "              torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wHidden,mean=0,std=self.sigmaNeu)\n",
    "              torch.nn.init.normal_(self.wHiddenBias,mean=0,std=self.sigmaNeu)\n",
    "            else:\n",
    "              torch.nn.init.xavier_uniform(self.wNeu)\n",
    "              torch.nn.init.xavier_uniform(self.wHidden)\n",
    "            \n",
    "  \n",
    "           \n",
    "\n",
    "        self.wNeu.requires_grad=True\n",
    "        self.wNeuBias.requires_grad=True\n",
    "        \n",
    "        self.dropout=torch.nn.Dropout(p=dropprob, inplace=False)\n",
    "        self.max=torch.nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        \n",
    "    def get_weights(self):\n",
    "        ll=[]\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "          for p in layer_p:\n",
    "            if 'weight' in p:\n",
    "               ll.append(self.rnn.__getattr__(p))\n",
    "        return ll+self.ConvWeights+self.ConvBias\n",
    "        \n",
    "    def layer1out(self,x):\n",
    "\t\t\n",
    "        if type(x) is np.ndarray:\n",
    "          x = torch.from_numpy(x.astype(np.float32))\n",
    "        #x = Variable(x, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "          x = x.to(device)\n",
    "        if embedding:\n",
    "          print(x.shape)\n",
    "          x= self.embedding(x)\n",
    "          x=x.permute(0,2,1)\n",
    "          print(x.shape)\n",
    "        if conv:\n",
    "          x=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
    "          out=x.clamp(min=0)\n",
    "          print(out.shape)\n",
    "          temp = out.data.cpu().numpy()\n",
    "        else:\n",
    "          print('you need to have CNN to visualize motifs')\n",
    "        return temp\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        if embedding:\n",
    "          # shape of x : batch_size x seq_len\n",
    "          x= self.embedding(x)\n",
    "          x=x.permute(0,2,1)\n",
    "          # shape of x_emb : batch_size x embd_size x seq_len\n",
    "          \n",
    "#         else:\n",
    "#           # shape of x : batch_size x 4 x seq_len\n",
    "         \n",
    "        if conv:\n",
    "          x=F.conv1d(x, self.ConvWeights[0], bias=self.ConvBias[0], stride=1, padding=0)\n",
    "          x=x.clamp(min=0)\n",
    "          x=self.max(x)\n",
    "          \n",
    "          for c in range(1,len(self.ConvWeights)):\n",
    "            x=F.conv1d(x, self.ConvWeights[c], bias=self.ConvBias[c], stride=1, padding=0,dilation=dilation)\n",
    "            x=x.clamp(min=0)\n",
    "            x=self.max(x)\n",
    "            \n",
    "        if RNN:\n",
    "          if conv:\n",
    "            \n",
    "            x=self.dropout(x)\n",
    "          x=x.permute(2,0,1)\n",
    "          # shape of x :  seq_len x batch_size  x features\n",
    "          output, _ = self.rnn(x)\n",
    "          # shape of output :  seq_len x batch_size  x num_directions * features\n",
    "          \n",
    "          if RNN_type== 'BiLSTM' or RNN_type=='BiGRU':\n",
    "\n",
    "              Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "              Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "              x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "              x=self.dropout(x)\n",
    "              #shape of x: batch_size x 2*hidden_size\n",
    "#               print(x.shape)\n",
    "              \n",
    "          else:\n",
    "                      ## from (1, N, hidden) to (N, hidden)\n",
    "              x = output[-1, :, :]\n",
    "              x=self.dropout(x)\n",
    "#               print(hn.shape)\n",
    "#               x = hn.view(hn.size()[1], hn.size(2))\n",
    "              # shape of x: batch_size x hidden_size\n",
    "              #print(x.shape)\n",
    "          \n",
    "          \n",
    "        else:\n",
    "          x, _ = torch.max(x, dim=2)\n",
    "          #print(x.shape)\n",
    "\n",
    "          # shape of x : batch_size x numb_filters\n",
    "          x=self.dropout(x)\n",
    "        if self.hidden:\n",
    "          x=x @ self.wHidden\n",
    "          x.add_(self.wHiddenBias)\n",
    "          x=x.clamp(min=0)\n",
    "          x=self.dropout(x)\n",
    "        x=x @ self.wNeu\n",
    "        x.add_(self.wNeuBias)\n",
    "          \n",
    "        \n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec model\n"
     ]
    }
   ],
   "source": [
    "stride = 1 # stride of sliding window\n",
    "Embepochs = 100 # number of iterations\n",
    "Embsize = 50 # embedding size\n",
    "print('training word2vec model')\n",
    "document= Gen_Words(train_data['acpM-kasA'], 25, 1)\n",
    "model = gensim.models.Word2Vec (document, window=int(12 / stride), min_count=0, vector_size=Embsize, workers=multiprocessing.cpu_count())\n",
    "model.train(document,total_examples=len(document),epochs=Embepochs)\n",
    "model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration():\n",
    "\tprint('start')\n",
    "\tbest_AUC=0\n",
    "\tdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\tprint(device)\n",
    "\t# device='cpu'\n",
    "\tlearning_steps_list=[5000,10000,15000,20000,25000,30000,35000,40000]\n",
    "\tfor number in range(40):\n",
    "\t\t# hyper-parameters\n",
    "\t\tRNN_hidden_size_list=[20,50,80,100]\n",
    "\t\tRNN_hidden_size=random.choice(RNN_hidden_size_list)\n",
    "\t\tdropoutList=[0,0.15,0.3,0.45,0.6] \n",
    "\t\tdropprob=random.choice(dropoutList)\n",
    "\t\thidden_list=[True,False]\n",
    "\t\thidden=random.choice(hidden_list)\n",
    "\t\txavier_List=[True,True,False] \n",
    "\t\txavier=random.choice(xavier_List)\n",
    "\t\thidden_size_list=[32,64]\n",
    "\t\thidden_size=random.choice(hidden_size_list)\n",
    "\t\toptim_list=['SGD','Adagrad','Adagrad']\n",
    "\t\toptim=random.choice(optim_list)\n",
    "\t\tlearning_rate=logsampler(0.005,0.5) \n",
    "\t\tmomentum_rate=sqrtsampler(0.95,0.99)  \n",
    "\t\tsigmaConv=logsampler(10**-6,10**-2)   \n",
    "\t\tsigmaNeu=logsampler(10**-3,10**-1) \n",
    "\t\tsigmaRNN=logsampler(10**-4,10**-1) \n",
    "\t\tweightDecay=logsampler(10**-10,10**-1) \n",
    "\t\tnummotif_list=[16]\n",
    "\t\tnummotif1=random.choice(nummotif_list)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\n",
    "\t\t\n",
    "\t\tmodel_auc=[[],[],[]]\n",
    "\t\tfor kk in range(3):\n",
    "\t\t\tmodel = Network(nummotif1,motiflen,RNN_hidden_size,hidden_size,hidden,dropprob,sigmaConv,sigmaNeu,sigmaRNN,xavier).to(device)\n",
    "\t\t\tif optim=='SGD':\n",
    "\t\t\t\toptimizer = torch.optim.SGD(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=learning_rate,momentum=momentum_rate,nesterov=True\n",
    "\t\t\t\t\t\t\t\t\t\t\t,weight_decay=weightDecay)\n",
    "\t\t\telse:\n",
    "\t\t\t\toptimizer = torch.optim.Adagrad(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=learning_rate,weight_decay=weightDecay)\n",
    "\n",
    "\t\t\ttrain_loader=train_dataloader[kk]\n",
    "\t\t\tvalid_loader=valid_dataloader[kk]\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tlearning_steps=0\n",
    "\t\t\twhile learning_steps<=40000:\n",
    "\t\t\t   \n",
    "\t\t\t\tauc=[]\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\t\tfor i, (data, target) in enumerate(train_loader):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tdata = data.to(device)\n",
    "\t\t\t\t\ttarget = target.to(device)\n",
    "\n",
    "\t\t\t\t\t# Forward pass\n",
    "\t\t\t\t\toutput = model(data)          \n",
    "\t\t\t\t\tloss = F.binary_cross_entropy(output,target)\n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\t\t\t\t\tlearning_steps+=1\n",
    "\n",
    "\t\t\t\t\tif learning_steps% 5000==0:\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t\t\tmodel.eval()\n",
    "\t\t\t\t\t\t\tauc=[]\n",
    "\t\t\t\t\t\t\tfor j, (data1, target1) in enumerate(valid_loader):\n",
    "\t\t\t\t\t\t\t\tdata1 = data1.to(device)\n",
    "\t\t\t\t\t\t\t\ttarget1 = target1.to(device)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t# Forward pass\n",
    "\t\t\t\t\t\t\t\toutput = model(data1)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tpred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "\t\t\t\t\t\t\t\tlabels=target1.cpu().numpy().reshape(output.shape[0])\n",
    "\t\t\t\t\t\t\t\tif output.shape[0]>60:\n",
    "\t\t\t\t\t\t\t\t\tauc.append(metrics.roc_auc_score(labels, pred))\n",
    "\t\t\t\t\t\t\t#print(np.mean(auc))\n",
    "\t\t\t\t\t\t\tmodel_auc[kk].append(np.mean(auc))\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tmodel.train()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\n",
    "\t\tprint('                   ##########################################               ')\n",
    "\n",
    "\t\tfor n in range(8):\n",
    "\t\t\tAUC=(model_auc[0][n]+model_auc[1][n]+model_auc[2][n])/3\n",
    "\t\t\t#print(AUC)\n",
    "\t\t\tif AUC>best_AUC:\n",
    "\t\t\t\tbest_AUC=AUC\n",
    "\t\t\t\tbest_learning_steps=learning_steps_list[n]\n",
    "\t\t\t\tbest_LearningRate=learning_rate\n",
    "\t\t\t\tbest_LearningMomentum=momentum_rate\n",
    "\t\t\t\tbest_sigmaConv=sigmaConv\n",
    "\t\t\t\tbest_dropprob=dropprob\n",
    "\t\t\t\tbest_sigmaNeu=sigmaNeu\n",
    "\t\t\t\tbest_RNN_hidden_size=RNN_hidden_size\n",
    "\t\t\t\tbest_weightDecay=weightDecay\n",
    "\t\t\t\tbest_hidden=hidden\n",
    "\t\t\t\tbest_sigmaRNN=sigmaRNN\n",
    "\t\t\t\tbest_xavier=xavier\n",
    "\t\t\t\tbest_optim=optim\n",
    "\t\t\t\tbest_nummotif=nummotif\n",
    "\t\t\t\tbest_hidden_size=hidden_size\n",
    "\n",
    "\n",
    "\tprint('best_AUC=',best_AUC)            \n",
    "\tprint('best_learning_steps=',best_learning_steps)      \n",
    "\tprint('best_LearningRate=',best_LearningRate)\n",
    "\tprint('best_LearningMomentum=',best_LearningMomentum)\n",
    "\tprint('best_sigmaConv=',best_sigmaConv)\n",
    "\tprint('best_dropprob=',best_dropprob)\n",
    "\tprint('best_sigmaNeu=',best_sigmaNeu)\n",
    "\tprint('best_RNN_hidden_size',best_RNN_hidden_size)\n",
    "\tprint('best_weightDecay=',weightDecay)\n",
    "\tprint('best_hidden=',best_hidden)\n",
    "\tprint('best_sigmaRNN=',best_sigmaRNN)\n",
    "\tprint('best_xavier=',best_xavier)\n",
    "\tprint('best_optim=',best_optim)\n",
    "\tprint('best_nummotif=',best_nummotif)\n",
    "\tprint('best_hidden_size=',best_hidden_size)\n",
    "\t\n",
    "\tbest_hyperparameters = {'best_learning_steps': best_learning_steps,'best_LearningRate':best_LearningRate,'best_LearningMomentum':best_LearningMomentum,'best_sigmaConv':best_sigmaConv,\n",
    "\t                         'best_dropprob':best_dropprob,'best_sigmaNeu':best_sigmaNeu,'best_RNN_hidden_size':best_RNN_hidden_size,\n",
    "\t                         'best_weightDecay':best_weightDecay,'best_hidden':best_hidden,'best_sigmaRNN':best_sigmaRNN,'best_xavier':best_xavier,'best_optim':best_optim,'best_nummotif':best_nummotif,'best_hidden_size':best_hidden_size}\n",
    "\ttorch.save(best_hyperparameters, model_dir+'best_hyperpamarameters.pth')\n",
    "\treturn best_hyperparameters\n",
    "def Train_model():\n",
    "\tbest_hyperparameters=torch.load(model_dir+'best_hyperpamarameters.pth')\n",
    "\tbest_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "\tbest_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "\tbest_LearningMomentum=best_hyperparameters['best_LearningMomentum']\n",
    "\tbest_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
    "\tbest_dropprob=best_hyperparameters['best_dropprob']\n",
    "\tbest_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
    "\tbest_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "\tbest_weightDecay=best_hyperparameters['best_weightDecay']\n",
    "\tbest_hidden=best_hyperparameters['best_hidden']\n",
    "\tbest_sigmaRNN=best_hyperparameters['best_sigmaRNN']\n",
    "\tbest_xavier=best_hyperparameters['best_xavier']\n",
    "\tbest_optim=best_hyperparameters['best_optim']\n",
    "\tbest_nummotif=best_hyperparameters['best_nummotif']\n",
    "\tbest_hidden_size=best_hyperparameters['best_hidden_size']\n",
    "\tbest_AUC=0\n",
    "\n",
    "\n",
    "\tfor number_models in range(5):\n",
    "\t  model = Network(best_nummotif,motiflen,best_RNN_hidden_size,best_hidden_size,best_hidden,best_dropprob,best_sigmaConv,best_sigmaNeu,best_sigmaRNN,best_xavier).to(device)\n",
    "\t  if best_optim=='SGD':\n",
    "\t\t  optimizer = torch.optim.SGD(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=best_LearningRate,momentum=best_LearningMomentum,nesterov=True,weight_decay=best_weightDecay)\n",
    "\t  else:\n",
    "\t\t  optimizer = torch.optim.Adagrad(model.get_weights()+[model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=best_LearningRate,weight_decay=best_weightDecay)\n",
    "\n",
    "\n",
    "\t  train_loader=alldataset_loader\n",
    "\t  valid_loader=alldataset_loader\n",
    "\t  learning_steps=0\n",
    "\t  model.train()\n",
    "\t  while learning_steps<=best_learning_steps:\n",
    "\t  \n",
    "\t\t  for i, (data, target) in enumerate(train_loader):\n",
    "\t\t\t  data = data.to(device)\n",
    "\t\t\t  target = target.to(device)\n",
    "\t\t\t  \n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t  output = model(data)\n",
    "\t\t\t  loss = F.binary_cross_entropy(output,target)\n",
    "\n",
    "\t\t\t  optimizer.zero_grad()\n",
    "\t\t\t  loss.backward()\n",
    "\t\t\t  optimizer.step()\n",
    "\t\t\t  learning_steps+=1\n",
    "\t\t\t  \n",
    "\t  with torch.no_grad():\n",
    "\t\t  model.eval()\n",
    "\t\t  auc=[]\n",
    "\t\t  for i, (data, target) in enumerate(valid_loader):\n",
    "\t\t\t  data = data.to(device)\n",
    "\t\t\t  target = target.to(device)\n",
    "\t\t\t  \n",
    "\t\t\t  # Forward pass\n",
    "\t\t\t  output = model(data)\n",
    "\t\t\t  \n",
    "\t\t\t  pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "\t\t\t  labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "\t\t\t  if output.shape[0]>30:\n",
    "\t\t\t\t  auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\t  #             \n",
    "\t\t  AUC_training=np.mean(auc)\n",
    "\t\t  print('AUC on training data for model ',number_models+1,' = ',AUC_training)\n",
    "\t\t  if AUC_training>best_AUC:\n",
    "\t\t\t  best_AUC=AUC_training\n",
    "\t\t\t  best_model=model\n",
    "\ttorch.save(best_model, model_path)\n",
    "\t#torch.save(best_model.state_dict(), model_dir+'best_model.pkl')\n",
    "\treturn best_model\n",
    "#### save model .pkl\n",
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-g1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
